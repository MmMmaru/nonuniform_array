{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62cfb08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "XTrain size: (11200, 34)\n",
      "train_data size: (11200, 10, 10) \n",
      "\n",
      "torch.Size([10, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\PROJECT\\dl\\ML2022-Spring\\.conda\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:608: UserWarning: Using a target size (torch.Size([256, 10, 10])) that is different to the input size (torch.Size([256, 1, 10, 10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "f:\\PROJECT\\dl\\ML2022-Spring\\.conda\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:608: UserWarning: Using a target size (torch.Size([192, 10, 10])) that is different to the input size (torch.Size([192, 1, 10, 10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "f:\\PROJECT\\dl\\ML2022-Spring\\.conda\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:608: UserWarning: Using a target size (torch.Size([128, 10, 10])) that is different to the input size (torch.Size([128, 1, 10, 10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500]: Train loss: 3.8582, Valid loss: 2.4942\n",
      "Epoch 1, 当前学习率: 0.001000\n",
      "Epoch [2/500]: Train loss: 1.7522, Valid loss: 1.6995\n",
      "Epoch 2, 当前学习率: 0.001000\n",
      "Epoch [3/500]: Train loss: 1.7333, Valid loss: 1.6899\n",
      "Epoch 3, 当前学习率: 0.001000\n",
      "Epoch [4/500]: Train loss: 1.7290, Valid loss: 1.6719\n",
      "Epoch 4, 当前学习率: 0.001000\n",
      "Epoch [5/500]: Train loss: 1.7290, Valid loss: 1.6870\n",
      "Epoch 5, 当前学习率: 0.001000\n",
      "Epoch [6/500]: Train loss: 1.7292, Valid loss: 1.6772\n",
      "Epoch 6, 当前学习率: 0.001000\n",
      "Epoch [7/500]: Train loss: 1.7274, Valid loss: 1.6821\n",
      "Epoch 7, 当前学习率: 0.001000\n",
      "Epoch [8/500]: Train loss: 1.7291, Valid loss: 1.7200\n",
      "Epoch 8, 当前学习率: 0.001000\n",
      "Epoch [9/500]: Train loss: 1.7286, Valid loss: 1.6842\n",
      "Epoch 9, 当前学习率: 0.001000\n",
      "Epoch [10/500]: Train loss: 1.7289, Valid loss: 1.6756\n",
      "Epoch 10, 当前学习率: 0.001000\n",
      "Epoch [11/500]: Train loss: 1.7299, Valid loss: 1.6796\n",
      "Epoch 11, 当前学习率: 0.001000\n",
      "Epoch [12/500]: Train loss: 1.7291, Valid loss: 1.7279\n",
      "Epoch 12, 当前学习率: 0.001000\n",
      "Epoch [13/500]: Train loss: 1.7296, Valid loss: 1.6905\n",
      "Epoch 13, 当前学习率: 0.001000\n",
      "Epoch [14/500]: Train loss: 1.7309, Valid loss: 1.6864\n",
      "Epoch 14, 当前学习率: 0.001000\n",
      "Epoch [15/500]: Train loss: 1.7308, Valid loss: 1.6788\n",
      "Epoch 15, 当前学习率: 0.001000\n",
      "Epoch [16/500]: Train loss: 1.7288, Valid loss: 1.6839\n",
      "Epoch 16, 当前学习率: 0.001000\n",
      "Epoch [17/500]: Train loss: 1.7283, Valid loss: 1.7007\n",
      "Epoch 17, 当前学习率: 0.001000\n",
      "Epoch [18/500]: Train loss: 1.7334, Valid loss: 1.6744\n",
      "Epoch 18, 当前学习率: 0.001000\n",
      "Epoch [19/500]: Train loss: 1.7309, Valid loss: 1.6899\n",
      "Epoch 19, 当前学习率: 0.001000\n",
      "Epoch [20/500]: Train loss: 1.7294, Valid loss: 1.7070\n",
      "Epoch 20, 当前学习率: 0.001000\n",
      "Epoch [21/500]: Train loss: 1.7336, Valid loss: 1.6713\n",
      "Epoch 21, 当前学习率: 0.001000\n",
      "Epoch [22/500]: Train loss: 1.7309, Valid loss: 1.6715\n",
      "Epoch 22, 当前学习率: 0.001000\n",
      "Epoch [23/500]: Train loss: 1.7300, Valid loss: 1.6872\n",
      "Epoch 23, 当前学习率: 0.001000\n",
      "Epoch [24/500]: Train loss: 1.7314, Valid loss: 1.6826\n",
      "Epoch 24, 当前学习率: 0.001000\n",
      "Epoch [25/500]: Train loss: 1.7319, Valid loss: 1.7386\n",
      "Epoch 25, 当前学习率: 0.001000\n",
      "Epoch [26/500]: Train loss: 1.7320, Valid loss: 1.6853\n",
      "Epoch 26, 当前学习率: 0.001000\n",
      "Epoch [27/500]: Train loss: 1.7287, Valid loss: 1.6890\n",
      "Epoch 27, 当前学习率: 0.001000\n",
      "Epoch [28/500]: Train loss: 1.7285, Valid loss: 1.6801\n",
      "Epoch 28, 当前学习率: 0.001000\n",
      "Epoch [29/500]: Train loss: 1.7304, Valid loss: 1.6734\n",
      "Epoch 29, 当前学习率: 0.001000\n",
      "Epoch [30/500]: Train loss: 1.7303, Valid loss: 1.6983\n",
      "Epoch 30, 当前学习率: 0.001000\n",
      "Epoch [31/500]: Train loss: 1.7315, Valid loss: 1.6961\n",
      "Epoch 31, 当前学习率: 0.001000\n",
      "Epoch [32/500]: Train loss: 1.7306, Valid loss: 1.6831\n",
      "Epoch 32, 当前学习率: 0.001000\n",
      "Epoch [33/500]: Train loss: 1.7292, Valid loss: 1.6930\n",
      "Epoch 33, 当前学习率: 0.001000\n",
      "Epoch [34/500]: Train loss: 1.7321, Valid loss: 1.6743\n",
      "Epoch 34, 当前学习率: 0.001000\n",
      "Epoch [35/500]: Train loss: 1.7307, Valid loss: 1.6771\n",
      "Epoch 35, 当前学习率: 0.001000\n",
      "Epoch [36/500]: Train loss: 1.7309, Valid loss: 1.6853\n",
      "Epoch 36, 当前学习率: 0.001000\n",
      "Epoch [37/500]: Train loss: 1.7312, Valid loss: 1.7092\n",
      "Epoch 37, 当前学习率: 0.001000\n",
      "Epoch [38/500]: Train loss: 1.7301, Valid loss: 1.6849\n",
      "Epoch 38, 当前学习率: 0.001000\n",
      "Epoch [39/500]: Train loss: 1.7330, Valid loss: 1.7087\n",
      "Epoch 39, 当前学习率: 0.001000\n",
      "Epoch [40/500]: Train loss: 1.7337, Valid loss: 1.6937\n",
      "Epoch 40, 当前学习率: 0.001000\n",
      "Epoch [41/500]: Train loss: 1.7309, Valid loss: 1.6923\n",
      "Epoch 41, 当前学习率: 0.001000\n",
      "Epoch [42/500]: Train loss: 1.7298, Valid loss: 1.6806\n",
      "Epoch 42, 当前学习率: 0.001000\n",
      "Epoch [43/500]: Train loss: 1.7307, Valid loss: 1.7610\n",
      "Epoch 43, 当前学习率: 0.001000\n",
      "Epoch [44/500]: Train loss: 1.7327, Valid loss: 1.6783\n",
      "Epoch 44, 当前学习率: 0.001000\n",
      "Epoch [45/500]: Train loss: 1.7342, Valid loss: 1.6958\n",
      "Epoch 45, 当前学习率: 0.001000\n",
      "Epoch [46/500]: Train loss: 1.7317, Valid loss: 1.6864\n",
      "Epoch 46, 当前学习率: 0.001000\n",
      "Epoch [47/500]: Train loss: 1.7327, Valid loss: 1.6811\n",
      "Epoch 47, 当前学习率: 0.001000\n",
      "Epoch [48/500]: Train loss: 1.7332, Valid loss: 1.7006\n",
      "Epoch 48, 当前学习率: 0.001000\n",
      "Epoch [49/500]: Train loss: 1.7312, Valid loss: 1.7009\n",
      "Epoch 49, 当前学习率: 0.001000\n",
      "Epoch [50/500]: Train loss: 1.7310, Valid loss: 1.6765\n",
      "Epoch 50, 当前学习率: 0.001000\n",
      "Epoch [51/500]: Train loss: 1.7313, Valid loss: 1.6863\n",
      "Epoch 51, 当前学习率: 0.001000\n",
      "Epoch [52/500]: Train loss: 1.7321, Valid loss: 1.6749\n",
      "Epoch 52, 当前学习率: 0.001000\n",
      "Epoch [53/500]: Train loss: 1.7307, Valid loss: 1.6824\n",
      "Epoch 53, 当前学习率: 0.001000\n",
      "Epoch [54/500]: Train loss: 1.7302, Valid loss: 1.6801\n",
      "Epoch 54, 当前学习率: 0.001000\n",
      "Epoch [55/500]: Train loss: 1.7323, Valid loss: 1.6806\n",
      "Epoch 55, 当前学习率: 0.001000\n",
      "Epoch [56/500]: Train loss: 1.7308, Valid loss: 1.6786\n",
      "Epoch 56, 当前学习率: 0.001000\n",
      "Epoch [57/500]: Train loss: 1.7293, Valid loss: 1.6773\n",
      "Epoch 57, 当前学习率: 0.001000\n",
      "Epoch [58/500]: Train loss: 1.7313, Valid loss: 1.6915\n",
      "Epoch 58, 当前学习率: 0.001000\n",
      "Epoch [59/500]: Train loss: 1.7338, Valid loss: 1.6767\n",
      "Epoch 59, 当前学习率: 0.001000\n",
      "Epoch [60/500]: Train loss: 1.7310, Valid loss: 1.6719\n",
      "Epoch 60, 当前学习率: 0.001000\n",
      "Epoch [61/500]: Train loss: 1.7300, Valid loss: 1.6722\n",
      "Epoch 61, 当前学习率: 0.001000\n",
      "Epoch [62/500]: Train loss: 1.7311, Valid loss: 1.6946\n",
      "Epoch 62, 当前学习率: 0.001000\n",
      "Epoch [63/500]: Train loss: 1.7308, Valid loss: 1.7114\n",
      "Epoch 63, 当前学习率: 0.001000\n",
      "Epoch [64/500]: Train loss: 1.7323, Valid loss: 1.6900\n",
      "Epoch 64, 当前学习率: 0.001000\n",
      "Epoch [65/500]: Train loss: 1.7298, Valid loss: 1.6778\n",
      "Epoch 65, 当前学习率: 0.001000\n",
      "Epoch [66/500]: Train loss: 1.7298, Valid loss: 1.7258\n",
      "Epoch 66, 当前学习率: 0.001000\n",
      "Epoch [67/500]: Train loss: 1.7317, Valid loss: 1.6807\n",
      "Epoch 67, 当前学习率: 0.001000\n",
      "Epoch [68/500]: Train loss: 1.7323, Valid loss: 1.6809\n",
      "Epoch 68, 当前学习率: 0.001000\n",
      "Epoch [69/500]: Train loss: 1.7302, Valid loss: 1.6921\n",
      "Epoch 69, 当前学习率: 0.001000\n",
      "Epoch [70/500]: Train loss: 1.7322, Valid loss: 1.6986\n",
      "Epoch 70, 当前学习率: 0.001000\n",
      "Epoch [71/500]: Train loss: 1.7296, Valid loss: 1.6852\n",
      "Epoch 71, 当前学习率: 0.001000\n",
      "Epoch [72/500]: Train loss: 1.7368, Valid loss: 1.6820\n",
      "Epoch 72, 当前学习率: 0.001000\n",
      "Epoch [73/500]: Train loss: 1.7306, Valid loss: 1.6868\n",
      "Epoch 73, 当前学习率: 0.001000\n",
      "Epoch [74/500]: Train loss: 1.7316, Valid loss: 1.6807\n",
      "Epoch 74, 当前学习率: 0.001000\n",
      "Epoch [75/500]: Train loss: 1.7298, Valid loss: 1.6820\n",
      "Epoch 75, 当前学习率: 0.001000\n",
      "Epoch [76/500]: Train loss: 1.7310, Valid loss: 1.6750\n",
      "Epoch 76, 当前学习率: 0.001000\n",
      "Epoch [77/500]: Train loss: 1.7307, Valid loss: 1.6810\n",
      "Epoch 77, 当前学习率: 0.001000\n",
      "Epoch [78/500]: Train loss: 1.7301, Valid loss: 1.6886\n",
      "Epoch 78, 当前学习率: 0.001000\n",
      "Epoch [79/500]: Train loss: 1.7308, Valid loss: 1.6684\n",
      "Epoch 79, 当前学习率: 0.001000\n",
      "Epoch [80/500]: Train loss: 1.7308, Valid loss: 1.6782\n",
      "Epoch 80, 当前学习率: 0.001000\n",
      "Epoch [81/500]: Train loss: 1.7299, Valid loss: 1.6768\n",
      "Epoch 81, 当前学习率: 0.001000\n",
      "Epoch [82/500]: Train loss: 1.7291, Valid loss: 1.6815\n",
      "Epoch 82, 当前学习率: 0.001000\n",
      "Epoch [83/500]: Train loss: 1.7326, Valid loss: 1.6948\n",
      "Epoch 83, 当前学习率: 0.001000\n",
      "Epoch [84/500]: Train loss: 1.7340, Valid loss: 1.6852\n",
      "Epoch 84, 当前学习率: 0.001000\n",
      "Epoch [85/500]: Train loss: 1.7335, Valid loss: 1.7026\n",
      "Epoch 85, 当前学习率: 0.001000\n",
      "Epoch [86/500]: Train loss: 1.7317, Valid loss: 1.6849\n",
      "Epoch 86, 当前学习率: 0.001000\n",
      "Epoch [87/500]: Train loss: 1.7317, Valid loss: 1.6926\n",
      "Epoch 87, 当前学习率: 0.001000\n",
      "Epoch [88/500]: Train loss: 1.7290, Valid loss: 1.6887\n",
      "Epoch 88, 当前学习率: 0.001000\n",
      "Epoch [89/500]: Train loss: 1.7333, Valid loss: 1.7269\n",
      "Epoch 89, 当前学习率: 0.001000\n",
      "Epoch [90/500]: Train loss: 1.7345, Valid loss: 1.6869\n",
      "Epoch 90, 当前学习率: 0.001000\n",
      "Epoch [91/500]: Train loss: 1.7315, Valid loss: 1.6824\n",
      "Epoch 91, 当前学习率: 0.001000\n",
      "Epoch [92/500]: Train loss: 1.7298, Valid loss: 1.6854\n",
      "Epoch 92, 当前学习率: 0.001000\n",
      "Epoch [93/500]: Train loss: 1.7295, Valid loss: 1.6841\n",
      "Epoch 93, 当前学习率: 0.001000\n",
      "Epoch [94/500]: Train loss: 1.7315, Valid loss: 1.6724\n",
      "Epoch 94, 当前学习率: 0.001000\n",
      "Epoch [95/500]: Train loss: 1.7295, Valid loss: 1.6687\n",
      "Epoch 95, 当前学习率: 0.001000\n",
      "Epoch [96/500]: Train loss: 1.7288, Valid loss: 1.6772\n",
      "Epoch 96, 当前学习率: 0.001000\n",
      "Epoch [97/500]: Train loss: 1.7301, Valid loss: 1.6788\n",
      "Epoch 97, 当前学习率: 0.001000\n",
      "Epoch [98/500]: Train loss: 1.7311, Valid loss: 1.6743\n",
      "Epoch 98, 当前学习率: 0.001000\n",
      "Epoch [99/500]: Train loss: 1.7315, Valid loss: 1.6832\n",
      "Epoch 99, 当前学习率: 0.001000\n",
      "Epoch [100/500]: Train loss: 1.7323, Valid loss: 1.6821\n",
      "Epoch 100, 当前学习率: 0.001000\n",
      "Epoch [101/500]: Train loss: 1.7309, Valid loss: 1.6797\n",
      "Epoch 101, 当前学习率: 0.001000\n",
      "Epoch [102/500]: Train loss: 1.7318, Valid loss: 1.6897\n",
      "Epoch 102, 当前学习率: 0.001000\n",
      "Epoch [103/500]: Train loss: 1.7321, Valid loss: 1.6924\n",
      "Epoch 103, 当前学习率: 0.001000\n",
      "Epoch [104/500]: Train loss: 1.7325, Valid loss: 1.6834\n",
      "Epoch 104, 当前学习率: 0.001000\n",
      "Epoch [105/500]: Train loss: 1.7321, Valid loss: 1.6919\n",
      "Epoch 105, 当前学习率: 0.001000\n",
      "Epoch [106/500]: Train loss: 1.7293, Valid loss: 1.6788\n",
      "Epoch 106, 当前学习率: 0.001000\n",
      "Epoch [107/500]: Train loss: 1.7287, Valid loss: 1.6808\n",
      "Epoch 107, 当前学习率: 0.001000\n",
      "Epoch [108/500]: Train loss: 1.7318, Valid loss: 1.6847\n",
      "Epoch 108, 当前学习率: 0.001000\n",
      "Epoch [109/500]: Train loss: 1.7329, Valid loss: 1.6862\n",
      "Epoch 109, 当前学习率: 0.001000\n",
      "Epoch [110/500]: Train loss: 1.7333, Valid loss: 1.6805\n",
      "Epoch 110, 当前学习率: 0.001000\n",
      "Epoch [111/500]: Train loss: 1.7306, Valid loss: 1.6834\n",
      "Epoch 111, 当前学习率: 0.001000\n",
      "Epoch [112/500]: Train loss: 1.7301, Valid loss: 1.6719\n",
      "Epoch 112, 当前学习率: 0.001000\n",
      "Epoch [113/500]: Train loss: 1.7306, Valid loss: 1.6877\n",
      "Epoch 113, 当前学习率: 0.001000\n",
      "Epoch [114/500]: Train loss: 1.7333, Valid loss: 1.6869\n",
      "Epoch 114, 当前学习率: 0.001000\n",
      "Epoch [115/500]: Train loss: 1.7307, Valid loss: 1.6854\n",
      "Epoch 115, 当前学习率: 0.001000\n",
      "Epoch [116/500]: Train loss: 1.7304, Valid loss: 1.6796\n",
      "Epoch 116, 当前学习率: 0.001000\n",
      "Epoch [117/500]: Train loss: 1.7287, Valid loss: 1.7090\n",
      "Epoch 117, 当前学习率: 0.001000\n",
      "Epoch [118/500]: Train loss: 1.7294, Valid loss: 1.6815\n",
      "Epoch 118, 当前学习率: 0.001000\n",
      "Epoch [119/500]: Train loss: 1.7300, Valid loss: 1.6718\n",
      "Epoch 119, 当前学习率: 0.001000\n",
      "Epoch [120/500]: Train loss: 1.7317, Valid loss: 1.6830\n",
      "Epoch 120, 当前学习率: 0.001000\n",
      "Epoch [121/500]: Train loss: 1.7315, Valid loss: 1.7086\n",
      "Epoch 121, 当前学习率: 0.001000\n",
      "Epoch [122/500]: Train loss: 1.7340, Valid loss: 1.6905\n",
      "Epoch 122, 当前学习率: 0.001000\n",
      "Epoch [123/500]: Train loss: 1.7324, Valid loss: 1.6998\n",
      "Epoch 123, 当前学习率: 0.001000\n",
      "Epoch [124/500]: Train loss: 1.7334, Valid loss: 1.6728\n",
      "Epoch 124, 当前学习率: 0.001000\n",
      "Epoch [125/500]: Train loss: 1.7325, Valid loss: 1.6884\n",
      "Epoch 125, 当前学习率: 0.001000\n",
      "Epoch [126/500]: Train loss: 1.7294, Valid loss: 1.6812\n",
      "Epoch 126, 当前学习率: 0.001000\n",
      "Epoch [127/500]: Train loss: 1.7294, Valid loss: 1.6943\n",
      "Epoch 127, 当前学习率: 0.001000\n",
      "Epoch [128/500]: Train loss: 1.7296, Valid loss: 1.6756\n",
      "Epoch 128, 当前学习率: 0.001000\n",
      "Epoch [129/500]: Train loss: 1.7316, Valid loss: 1.6957\n",
      "Epoch 129, 当前学习率: 0.001000\n",
      "Epoch [130/500]: Train loss: 1.7314, Valid loss: 1.6984\n",
      "Epoch 130, 当前学习率: 0.001000\n",
      "Epoch [131/500]: Train loss: 1.7323, Valid loss: 1.6693\n",
      "Epoch 131, 当前学习率: 0.001000\n",
      "Epoch [132/500]: Train loss: 1.7309, Valid loss: 1.6805\n",
      "Epoch 132, 当前学习率: 0.001000\n",
      "Epoch [133/500]: Train loss: 1.7313, Valid loss: 1.6793\n",
      "Epoch 133, 当前学习率: 0.001000\n",
      "Epoch [134/500]: Train loss: 1.7294, Valid loss: 1.6936\n",
      "Epoch 134, 当前学习率: 0.001000\n",
      "Epoch [135/500]: Train loss: 1.7318, Valid loss: 1.6953\n",
      "Epoch 135, 当前学习率: 0.001000\n",
      "Epoch [136/500]: Train loss: 1.7313, Valid loss: 1.6897\n",
      "Epoch 136, 当前学习率: 0.001000\n",
      "Epoch [137/500]: Train loss: 1.7335, Valid loss: 1.6892\n",
      "Epoch 137, 当前学习率: 0.001000\n",
      "Epoch [138/500]: Train loss: 1.7295, Valid loss: 1.6763\n",
      "Epoch 138, 当前学习率: 0.001000\n",
      "Epoch [139/500]: Train loss: 1.7292, Valid loss: 1.6829\n",
      "Epoch 139, 当前学习率: 0.001000\n",
      "Epoch [140/500]: Train loss: 1.7317, Valid loss: 1.6783\n",
      "Epoch 140, 当前学习率: 0.001000\n",
      "Epoch [141/500]: Train loss: 1.7331, Valid loss: 1.6689\n",
      "Epoch 141, 当前学习率: 0.001000\n",
      "Epoch [142/500]: Train loss: 1.7309, Valid loss: 1.6910\n",
      "Epoch 142, 当前学习率: 0.001000\n",
      "Epoch [143/500]: Train loss: 1.7302, Valid loss: 1.6852\n",
      "Epoch 143, 当前学习率: 0.001000\n",
      "Epoch [144/500]: Train loss: 1.7316, Valid loss: 1.6691\n",
      "Epoch 144, 当前学习率: 0.001000\n",
      "Epoch [145/500]: Train loss: 1.7306, Valid loss: 1.6985\n",
      "Epoch 145, 当前学习率: 0.001000\n",
      "Epoch [146/500]: Train loss: 1.7295, Valid loss: 1.6826\n",
      "Epoch 146, 当前学习率: 0.001000\n",
      "Epoch [147/500]: Train loss: 1.7298, Valid loss: 1.6898\n",
      "Epoch 147, 当前学习率: 0.001000\n",
      "Epoch [148/500]: Train loss: 1.7307, Valid loss: 1.7011\n",
      "Epoch 148, 当前学习率: 0.001000\n",
      "Epoch [149/500]: Train loss: 1.7318, Valid loss: 1.6806\n",
      "Epoch 149, 当前学习率: 0.001000\n",
      "Epoch [150/500]: Train loss: 1.7309, Valid loss: 1.6914\n",
      "Epoch 150, 当前学习率: 0.001000\n",
      "Epoch [151/500]: Train loss: 1.7295, Valid loss: 1.6979\n",
      "Epoch 151, 当前学习率: 0.001000\n",
      "Epoch [152/500]: Train loss: 1.7309, Valid loss: 1.6922\n",
      "Epoch 152, 当前学习率: 0.001000\n",
      "Epoch [153/500]: Train loss: 1.7324, Valid loss: 1.6630\n",
      "Epoch 153, 当前学习率: 0.001000\n",
      "Epoch [154/500]: Train loss: 1.7286, Valid loss: 1.6825\n",
      "Epoch 154, 当前学习率: 0.001000\n",
      "Epoch [155/500]: Train loss: 1.7296, Valid loss: 1.6818\n",
      "Epoch 155, 当前学习率: 0.001000\n",
      "Epoch [156/500]: Train loss: 1.7316, Valid loss: 1.7006\n",
      "Epoch 156, 当前学习率: 0.001000\n",
      "Epoch [157/500]: Train loss: 1.7291, Valid loss: 1.6858\n",
      "Epoch 157, 当前学习率: 0.001000\n",
      "Epoch [158/500]: Train loss: 1.7291, Valid loss: 1.6894\n",
      "Epoch 158, 当前学习率: 0.001000\n",
      "Epoch [159/500]: Train loss: 1.7317, Valid loss: 1.6809\n",
      "Epoch 159, 当前学习率: 0.001000\n",
      "Epoch [160/500]: Train loss: 1.7313, Valid loss: 1.6739\n",
      "Epoch 160, 当前学习率: 0.001000\n",
      "Epoch [161/500]: Train loss: 1.7286, Valid loss: 1.6896\n",
      "Epoch 161, 当前学习率: 0.001000\n",
      "Epoch [162/500]: Train loss: 1.7298, Valid loss: 1.6916\n",
      "Epoch 162, 当前学习率: 0.001000\n",
      "Epoch [163/500]: Train loss: 1.7293, Valid loss: 1.6907\n",
      "Epoch 163, 当前学习率: 0.001000\n",
      "Epoch [164/500]: Train loss: 1.7285, Valid loss: 1.6874\n",
      "Epoch 164, 当前学习率: 0.001000\n",
      "Epoch [165/500]: Train loss: 1.7309, Valid loss: 1.6893\n",
      "Epoch 165, 当前学习率: 0.001000\n",
      "Epoch [166/500]: Train loss: 1.7301, Valid loss: 1.6772\n",
      "Epoch 166, 当前学习率: 0.001000\n",
      "Epoch [167/500]: Train loss: 1.7291, Valid loss: 1.6844\n",
      "Epoch 167, 当前学习率: 0.001000\n",
      "Epoch [168/500]: Train loss: 1.7307, Valid loss: 1.6887\n",
      "Epoch 168, 当前学习率: 0.001000\n",
      "Epoch [169/500]: Train loss: 1.7284, Valid loss: 1.6858\n",
      "Epoch 169, 当前学习率: 0.001000\n",
      "Epoch [170/500]: Train loss: 1.7315, Valid loss: 1.6779\n",
      "Epoch 170, 当前学习率: 0.001000\n",
      "Epoch [171/500]: Train loss: 1.7296, Valid loss: 1.6850\n",
      "Epoch 171, 当前学习率: 0.001000\n",
      "Epoch [172/500]: Train loss: 1.7300, Valid loss: 1.6920\n",
      "Epoch 172, 当前学习率: 0.001000\n",
      "Epoch [173/500]: Train loss: 1.7302, Valid loss: 1.6691\n",
      "Epoch 173, 当前学习率: 0.001000\n",
      "Epoch [174/500]: Train loss: 1.7280, Valid loss: 1.6781\n",
      "Epoch 174, 当前学习率: 0.001000\n",
      "Epoch [175/500]: Train loss: 1.7304, Valid loss: 1.7029\n",
      "Epoch 175, 当前学习率: 0.001000\n",
      "Epoch [176/500]: Train loss: 1.7293, Valid loss: 1.6727\n",
      "Epoch 176, 当前学习率: 0.001000\n",
      "Epoch [177/500]: Train loss: 1.7302, Valid loss: 1.6851\n",
      "Epoch 177, 当前学习率: 0.001000\n",
      "Epoch [178/500]: Train loss: 1.7313, Valid loss: 1.6732\n",
      "Epoch 178, 当前学习率: 0.001000\n",
      "Epoch [179/500]: Train loss: 1.7299, Valid loss: 1.6782\n",
      "Epoch 179, 当前学习率: 0.001000\n",
      "Epoch [180/500]: Train loss: 1.7298, Valid loss: 1.6691\n",
      "Epoch 180, 当前学习率: 0.001000\n",
      "Epoch [181/500]: Train loss: 1.7309, Valid loss: 1.6923\n",
      "Epoch 181, 当前学习率: 0.001000\n",
      "Epoch [182/500]: Train loss: 1.7289, Valid loss: 1.6806\n",
      "Epoch 182, 当前学习率: 0.001000\n",
      "Epoch [183/500]: Train loss: 1.7306, Valid loss: 1.6769\n",
      "Epoch 183, 当前学习率: 0.001000\n",
      "Epoch [184/500]: Train loss: 1.7292, Valid loss: 1.6795\n",
      "Epoch 184, 当前学习率: 0.001000\n",
      "Epoch [185/500]: Train loss: 1.7291, Valid loss: 1.6898\n",
      "Epoch 185, 当前学习率: 0.001000\n",
      "Epoch [186/500]: Train loss: 1.7289, Valid loss: 1.6811\n",
      "Epoch 186, 当前学习率: 0.001000\n",
      "Epoch [187/500]: Train loss: 1.7309, Valid loss: 1.6879\n",
      "Epoch 187, 当前学习率: 0.001000\n",
      "Epoch [188/500]: Train loss: 1.7312, Valid loss: 1.6837\n",
      "Epoch 188, 当前学习率: 0.001000\n",
      "Epoch [189/500]: Train loss: 1.7316, Valid loss: 1.6865\n",
      "Epoch 189, 当前学习率: 0.001000\n",
      "Epoch [190/500]: Train loss: 1.7293, Valid loss: 1.7036\n",
      "Epoch 190, 当前学习率: 0.001000\n",
      "Epoch [191/500]: Train loss: 1.7288, Valid loss: 1.6851\n",
      "Epoch 191, 当前学习率: 0.001000\n",
      "Epoch [192/500]: Train loss: 1.7303, Valid loss: 1.6855\n",
      "Epoch 192, 当前学习率: 0.001000\n",
      "Epoch [193/500]: Train loss: 1.7295, Valid loss: 1.6801\n",
      "Epoch 193, 当前学习率: 0.001000\n",
      "Epoch [194/500]: Train loss: 1.7295, Valid loss: 1.6855\n",
      "Epoch 194, 当前学习率: 0.001000\n",
      "Epoch [195/500]: Train loss: 1.7318, Valid loss: 1.6807\n",
      "Epoch 195, 当前学习率: 0.001000\n",
      "Epoch [196/500]: Train loss: 1.7354, Valid loss: 1.6823\n",
      "Epoch 196, 当前学习率: 0.001000\n",
      "Epoch [197/500]: Train loss: 1.7337, Valid loss: 1.6703\n",
      "Epoch 197, 当前学习率: 0.001000\n",
      "Epoch [198/500]: Train loss: 1.7293, Valid loss: 1.6852\n",
      "Epoch 198, 当前学习率: 0.001000\n",
      "Epoch [199/500]: Train loss: 1.7288, Valid loss: 1.6777\n",
      "Epoch 199, 当前学习率: 0.001000\n",
      "Epoch [200/500]: Train loss: 1.7312, Valid loss: 1.6833\n",
      "Epoch 200, 当前学习率: 0.001000\n",
      "Epoch [201/500]: Train loss: 1.7291, Valid loss: 1.6798\n",
      "Epoch 201, 当前学习率: 0.001000\n",
      "Epoch [202/500]: Train loss: 1.7295, Valid loss: 1.6805\n",
      "Epoch 202, 当前学习率: 0.001000\n",
      "Epoch [203/500]: Train loss: 1.7307, Valid loss: 1.6744\n",
      "Epoch 203, 当前学习率: 0.001000\n",
      "Epoch [204/500]: Train loss: 1.7297, Valid loss: 1.6830\n",
      "Epoch 204, 当前学习率: 0.001000\n",
      "Epoch [205/500]: Train loss: 1.7297, Valid loss: 1.6824\n",
      "Epoch 205, 当前学习率: 0.001000\n",
      "Epoch [206/500]: Train loss: 1.7285, Valid loss: 1.6770\n",
      "Epoch 206, 当前学习率: 0.001000\n",
      "Epoch [207/500]: Train loss: 1.7298, Valid loss: 1.6817\n",
      "Epoch 207, 当前学习率: 0.001000\n",
      "Epoch [208/500]: Train loss: 1.7314, Valid loss: 1.6930\n",
      "Epoch 208, 当前学习率: 0.001000\n",
      "Epoch [209/500]: Train loss: 1.7299, Valid loss: 1.6816\n",
      "Epoch 209, 当前学习率: 0.001000\n",
      "Epoch [210/500]: Train loss: 1.7286, Valid loss: 1.6887\n",
      "Epoch 210, 当前学习率: 0.001000\n",
      "Epoch [211/500]: Train loss: 1.7313, Valid loss: 1.6852\n",
      "Epoch 211, 当前学习率: 0.001000\n",
      "Epoch [212/500]: Train loss: 1.7283, Valid loss: 1.6859\n",
      "Epoch 212, 当前学习率: 0.001000\n",
      "Epoch [213/500]: Train loss: 1.7293, Valid loss: 1.6809\n",
      "Epoch 213, 当前学习率: 0.001000\n",
      "Epoch [214/500]: Train loss: 1.7298, Valid loss: 1.6705\n",
      "Epoch 214, 当前学习率: 0.001000\n",
      "Epoch [215/500]: Train loss: 1.7297, Valid loss: 1.6925\n",
      "Epoch 215, 当前学习率: 0.001000\n",
      "Epoch [216/500]: Train loss: 1.7296, Valid loss: 1.6859\n",
      "Epoch 216, 当前学习率: 0.001000\n",
      "Epoch [217/500]: Train loss: 1.7290, Valid loss: 1.6995\n",
      "Epoch 217, 当前学习率: 0.001000\n",
      "Epoch [218/500]: Train loss: 1.7289, Valid loss: 1.6805\n",
      "Epoch 218, 当前学习率: 0.001000\n",
      "Epoch [219/500]: Train loss: 1.7308, Valid loss: 1.6801\n",
      "Epoch 219, 当前学习率: 0.001000\n",
      "Epoch [220/500]: Train loss: 1.7304, Valid loss: 1.6871\n",
      "Epoch 220, 当前学习率: 0.001000\n",
      "Epoch [221/500]: Train loss: 1.7308, Valid loss: 1.6784\n",
      "Epoch 221, 当前学习率: 0.001000\n",
      "Epoch [222/500]: Train loss: 1.7289, Valid loss: 1.6857\n",
      "Epoch 222, 当前学习率: 0.001000\n",
      "Epoch [223/500]: Train loss: 1.7279, Valid loss: 1.6859\n",
      "Epoch 223, 当前学习率: 0.001000\n",
      "Epoch [224/500]: Train loss: 1.7322, Valid loss: 1.6936\n",
      "Epoch 224, 当前学习率: 0.001000\n",
      "Epoch [225/500]: Train loss: 1.7307, Valid loss: 1.6818\n",
      "Epoch 225, 当前学习率: 0.001000\n",
      "Epoch [226/500]: Train loss: 1.7298, Valid loss: 1.6919\n",
      "Epoch 226, 当前学习率: 0.001000\n",
      "Epoch [227/500]: Train loss: 1.7283, Valid loss: 1.6804\n",
      "Epoch 227, 当前学习率: 0.001000\n",
      "Epoch [228/500]: Train loss: 1.7297, Valid loss: 1.6858\n",
      "Epoch 228, 当前学习率: 0.001000\n",
      "Epoch [229/500]: Train loss: 1.7288, Valid loss: 1.6794\n",
      "Epoch 229, 当前学习率: 0.001000\n",
      "Epoch [230/500]: Train loss: 1.7292, Valid loss: 1.6830\n",
      "Epoch 230, 当前学习率: 0.001000\n",
      "Epoch [231/500]: Train loss: 1.7281, Valid loss: 1.6775\n",
      "Epoch 231, 当前学习率: 0.001000\n",
      "Epoch [232/500]: Train loss: 1.7295, Valid loss: 1.6783\n",
      "Epoch 232, 当前学习率: 0.001000\n",
      "Epoch [233/500]: Train loss: 1.7291, Valid loss: 1.6837\n",
      "Epoch 233, 当前学习率: 0.001000\n",
      "Epoch [234/500]: Train loss: 1.7287, Valid loss: 1.6800\n",
      "Epoch 234, 当前学习率: 0.001000\n",
      "Epoch [235/500]: Train loss: 1.7296, Valid loss: 1.6873\n",
      "Epoch 235, 当前学习率: 0.001000\n",
      "Epoch [236/500]: Train loss: 1.7303, Valid loss: 1.6910\n",
      "Epoch 236, 当前学习率: 0.001000\n",
      "Epoch [237/500]: Train loss: 1.7294, Valid loss: 1.6775\n",
      "Epoch 237, 当前学习率: 0.001000\n",
      "Epoch [238/500]: Train loss: 1.7300, Valid loss: 1.6738\n",
      "Epoch 238, 当前学习率: 0.001000\n",
      "Epoch [239/500]: Train loss: 1.7287, Valid loss: 1.6877\n",
      "Epoch 239, 当前学习率: 0.001000\n",
      "Epoch [240/500]: Train loss: 1.7280, Valid loss: 1.6851\n",
      "Epoch 240, 当前学习率: 0.001000\n",
      "Epoch [241/500]: Train loss: 1.7291, Valid loss: 1.6767\n",
      "Epoch 241, 当前学习率: 0.001000\n",
      "Epoch [242/500]: Train loss: 1.7303, Valid loss: 1.6826\n",
      "Epoch 242, 当前学习率: 0.001000\n",
      "Epoch [243/500]: Train loss: 1.7306, Valid loss: 1.6892\n",
      "Epoch 243, 当前学习率: 0.001000\n",
      "Epoch [244/500]: Train loss: 1.7316, Valid loss: 1.6691\n",
      "Epoch 244, 当前学习率: 0.001000\n",
      "Epoch [245/500]: Train loss: 1.7291, Valid loss: 1.6861\n",
      "Epoch 245, 当前学习率: 0.001000\n",
      "Epoch [246/500]: Train loss: 1.7296, Valid loss: 1.6738\n",
      "Epoch 246, 当前学习率: 0.001000\n",
      "Epoch [247/500]: Train loss: 1.7326, Valid loss: 1.6920\n",
      "Epoch 247, 当前学习率: 0.001000\n",
      "Epoch [248/500]: Train loss: 1.7295, Valid loss: 1.6868\n",
      "Epoch 248, 当前学习率: 0.001000\n",
      "Epoch [249/500]: Train loss: 1.7296, Valid loss: 1.6864\n",
      "Epoch 249, 当前学习率: 0.001000\n",
      "Epoch [250/500]: Train loss: 1.7302, Valid loss: 1.6782\n",
      "Epoch 250, 当前学习率: 0.001000\n",
      "Epoch [251/500]: Train loss: 1.7283, Valid loss: 1.6778\n",
      "Epoch 251, 当前学习率: 0.001000\n",
      "Epoch [252/500]: Train loss: 1.7295, Valid loss: 1.6688\n",
      "Epoch 252, 当前学习率: 0.001000\n",
      "Epoch [253/500]: Train loss: 1.7312, Valid loss: 1.6892\n",
      "Epoch 253, 当前学习率: 0.001000\n",
      "Epoch [254/500]: Train loss: 1.7302, Valid loss: 1.6877\n",
      "Epoch 254, 当前学习率: 0.000100\n",
      "Epoch [255/500]: Train loss: 1.7265, Valid loss: 1.6796\n",
      "Epoch 255, 当前学习率: 0.000100\n",
      "Epoch [256/500]: Train loss: 1.7255, Valid loss: 1.6957\n",
      "Epoch 256, 当前学习率: 0.000100\n",
      "Epoch [257/500]: Train loss: 1.7259, Valid loss: 1.6758\n",
      "Epoch 257, 当前学习率: 0.000100\n",
      "Epoch [258/500]: Train loss: 1.7256, Valid loss: 1.6822\n",
      "Epoch 258, 当前学习率: 0.000100\n",
      "Epoch [259/500]: Train loss: 1.7258, Valid loss: 1.6712\n",
      "Epoch 259, 当前学习率: 0.000100\n",
      "Epoch [260/500]: Train loss: 1.7257, Valid loss: 1.6784\n",
      "Epoch 260, 当前学习率: 0.000100\n",
      "Epoch [261/500]: Train loss: 1.7263, Valid loss: 1.6809\n",
      "Epoch 261, 当前学习率: 0.000100\n",
      "Epoch [262/500]: Train loss: 1.7261, Valid loss: 1.6815\n",
      "Epoch 262, 当前学习率: 0.000100\n",
      "Epoch [263/500]: Train loss: 1.7259, Valid loss: 1.6804\n",
      "Epoch 263, 当前学习率: 0.000100\n",
      "Epoch [264/500]: Train loss: 1.7258, Valid loss: 1.6789\n",
      "Epoch 264, 当前学习率: 0.000100\n",
      "Epoch [265/500]: Train loss: 1.7259, Valid loss: 1.6854\n",
      "Epoch 265, 当前学习率: 0.000100\n",
      "Epoch [266/500]: Train loss: 1.7264, Valid loss: 1.6759\n",
      "Epoch 266, 当前学习率: 0.000100\n",
      "Epoch [267/500]: Train loss: 1.7262, Valid loss: 1.6697\n",
      "Epoch 267, 当前学习率: 0.000100\n",
      "Epoch [268/500]: Train loss: 1.7261, Valid loss: 1.6764\n",
      "Epoch 268, 当前学习率: 0.000100\n",
      "Epoch [269/500]: Train loss: 1.7260, Valid loss: 1.6722\n",
      "Epoch 269, 当前学习率: 0.000100\n",
      "Epoch [270/500]: Train loss: 1.7259, Valid loss: 1.6747\n",
      "Epoch 270, 当前学习率: 0.000100\n",
      "Epoch [271/500]: Train loss: 1.7258, Valid loss: 1.6752\n",
      "Epoch 271, 当前学习率: 0.000100\n",
      "Epoch [272/500]: Train loss: 1.7263, Valid loss: 1.6701\n",
      "Epoch 272, 当前学习率: 0.000100\n",
      "Epoch [273/500]: Train loss: 1.7260, Valid loss: 1.6748\n",
      "Epoch 273, 当前学习率: 0.000100\n",
      "Epoch [274/500]: Train loss: 1.7257, Valid loss: 1.6741\n",
      "Epoch 274, 当前学习率: 0.000100\n",
      "Epoch [275/500]: Train loss: 1.7256, Valid loss: 1.6877\n",
      "Epoch 275, 当前学习率: 0.000100\n",
      "Epoch [276/500]: Train loss: 1.7262, Valid loss: 1.6880\n",
      "Epoch 276, 当前学习率: 0.000100\n",
      "Epoch [277/500]: Train loss: 1.7260, Valid loss: 1.6736\n",
      "Epoch 277, 当前学习率: 0.000100\n",
      "Epoch [278/500]: Train loss: 1.7259, Valid loss: 1.6767\n",
      "Epoch 278, 当前学习率: 0.000100\n",
      "Epoch [279/500]: Train loss: 1.7262, Valid loss: 1.6897\n",
      "Epoch 279, 当前学习率: 0.000100\n",
      "Epoch [280/500]: Train loss: 1.7262, Valid loss: 1.6748\n",
      "Epoch 280, 当前学习率: 0.000100\n",
      "Epoch [281/500]: Train loss: 1.7259, Valid loss: 1.6771\n",
      "Epoch 281, 当前学习率: 0.000100\n",
      "Epoch [282/500]: Train loss: 1.7257, Valid loss: 1.6816\n",
      "Epoch 282, 当前学习率: 0.000100\n",
      "Epoch [283/500]: Train loss: 1.7260, Valid loss: 1.6634\n",
      "Epoch 283, 当前学习率: 0.000100\n",
      "Epoch [284/500]: Train loss: 1.7259, Valid loss: 1.6826\n",
      "Epoch 284, 当前学习率: 0.000100\n",
      "Epoch [285/500]: Train loss: 1.7257, Valid loss: 1.6749\n",
      "Epoch 285, 当前学习率: 0.000100\n",
      "Epoch [286/500]: Train loss: 1.7265, Valid loss: 1.6885\n",
      "Epoch 286, 当前学习率: 0.000100\n",
      "Epoch [287/500]: Train loss: 1.7265, Valid loss: 1.6856\n",
      "Epoch 287, 当前学习率: 0.000100\n",
      "Epoch [288/500]: Train loss: 1.7260, Valid loss: 1.6865\n",
      "Epoch 288, 当前学习率: 0.000100\n",
      "Epoch [289/500]: Train loss: 1.7261, Valid loss: 1.6718\n",
      "Epoch 289, 当前学习率: 0.000100\n",
      "Epoch [290/500]: Train loss: 1.7258, Valid loss: 1.6757\n",
      "Epoch 290, 当前学习率: 0.000100\n",
      "Epoch [291/500]: Train loss: 1.7258, Valid loss: 1.6792\n",
      "Epoch 291, 当前学习率: 0.000100\n",
      "Epoch [292/500]: Train loss: 1.7255, Valid loss: 1.6711\n",
      "Epoch 292, 当前学习率: 0.000100\n",
      "Epoch [293/500]: Train loss: 1.7257, Valid loss: 1.6713\n",
      "Epoch 293, 当前学习率: 0.000100\n",
      "Epoch [294/500]: Train loss: 1.7260, Valid loss: 1.6677\n",
      "Epoch 294, 当前学习率: 0.000100\n",
      "Epoch [295/500]: Train loss: 1.7260, Valid loss: 1.6779\n",
      "Epoch 295, 当前学习率: 0.000100\n",
      "Epoch [296/500]: Train loss: 1.7264, Valid loss: 1.6847\n",
      "Epoch 296, 当前学习率: 0.000100\n",
      "Epoch [297/500]: Train loss: 1.7261, Valid loss: 1.6789\n",
      "Epoch 297, 当前学习率: 0.000100\n",
      "Epoch [298/500]: Train loss: 1.7258, Valid loss: 1.6749\n",
      "Epoch 298, 当前学习率: 0.000100\n",
      "Epoch [299/500]: Train loss: 1.7261, Valid loss: 1.6794\n",
      "Epoch 299, 当前学习率: 0.000100\n",
      "Epoch [300/500]: Train loss: 1.7257, Valid loss: 1.6803\n",
      "Epoch 300, 当前学习率: 0.000100\n",
      "Epoch [301/500]: Train loss: 1.7259, Valid loss: 1.6701\n",
      "Epoch 301, 当前学习率: 0.000100\n",
      "Epoch [302/500]: Train loss: 1.7259, Valid loss: 1.6752\n",
      "Epoch 302, 当前学习率: 0.000100\n",
      "Epoch [303/500]: Train loss: 1.7262, Valid loss: 1.6732\n",
      "Epoch 303, 当前学习率: 0.000100\n",
      "Epoch [304/500]: Train loss: 1.7259, Valid loss: 1.6792\n",
      "Epoch 304, 当前学习率: 0.000100\n",
      "Epoch [305/500]: Train loss: 1.7259, Valid loss: 1.6797\n",
      "Epoch 305, 当前学习率: 0.000100\n",
      "Epoch [306/500]: Train loss: 1.7265, Valid loss: 1.6753\n",
      "Epoch 306, 当前学习率: 0.000100\n",
      "Epoch [307/500]: Train loss: 1.7262, Valid loss: 1.6913\n",
      "Epoch 307, 当前学习率: 0.000100\n",
      "Epoch [308/500]: Train loss: 1.7263, Valid loss: 1.6783\n",
      "Epoch 308, 当前学习率: 0.000100\n",
      "Epoch [309/500]: Train loss: 1.7258, Valid loss: 1.6752\n",
      "Epoch 309, 当前学习率: 0.000100\n",
      "Epoch [310/500]: Train loss: 1.7260, Valid loss: 1.6821\n",
      "Epoch 310, 当前学习率: 0.000100\n",
      "Epoch [311/500]: Train loss: 1.7263, Valid loss: 1.6719\n",
      "Epoch 311, 当前学习率: 0.000100\n",
      "Epoch [312/500]: Train loss: 1.7263, Valid loss: 1.6822\n",
      "Epoch 312, 当前学习率: 0.000100\n",
      "Epoch [313/500]: Train loss: 1.7261, Valid loss: 1.6781\n",
      "Epoch 313, 当前学习率: 0.000100\n",
      "Epoch [314/500]: Train loss: 1.7262, Valid loss: 1.6845\n",
      "Epoch 314, 当前学习率: 0.000100\n",
      "Epoch [315/500]: Train loss: 1.7257, Valid loss: 1.6755\n",
      "Epoch 315, 当前学习率: 0.000100\n",
      "Epoch [316/500]: Train loss: 1.7261, Valid loss: 1.6721\n",
      "Epoch 316, 当前学习率: 0.000100\n",
      "Epoch [317/500]: Train loss: 1.7257, Valid loss: 1.6720\n",
      "Epoch 317, 当前学习率: 0.000100\n",
      "Epoch [318/500]: Train loss: 1.7262, Valid loss: 1.6666\n",
      "Epoch 318, 当前学习率: 0.000100\n",
      "Epoch [319/500]: Train loss: 1.7259, Valid loss: 1.6760\n",
      "Epoch 319, 当前学习率: 0.000100\n",
      "Epoch [320/500]: Train loss: 1.7259, Valid loss: 1.6759\n",
      "Epoch 320, 当前学习率: 0.000100\n",
      "Epoch [321/500]: Train loss: 1.7260, Valid loss: 1.6938\n",
      "Epoch 321, 当前学习率: 0.000100\n",
      "Epoch [322/500]: Train loss: 1.7264, Valid loss: 1.6765\n",
      "Epoch 322, 当前学习率: 0.000100\n",
      "Epoch [323/500]: Train loss: 1.7258, Valid loss: 1.6694\n",
      "Epoch 323, 当前学习率: 0.000100\n",
      "Epoch [324/500]: Train loss: 1.7260, Valid loss: 1.6874\n",
      "Epoch 324, 当前学习率: 0.000100\n",
      "Epoch [325/500]: Train loss: 1.7261, Valid loss: 1.6842\n",
      "Epoch 325, 当前学习率: 0.000100\n",
      "Epoch [326/500]: Train loss: 1.7263, Valid loss: 1.6770\n",
      "Epoch 326, 当前学习率: 0.000100\n",
      "Epoch [327/500]: Train loss: 1.7262, Valid loss: 1.6754\n",
      "Epoch 327, 当前学习率: 0.000100\n",
      "Epoch [328/500]: Train loss: 1.7257, Valid loss: 1.6787\n",
      "Epoch 328, 当前学习率: 0.000100\n",
      "Epoch [329/500]: Train loss: 1.7264, Valid loss: 1.6846\n",
      "Epoch 329, 当前学习率: 0.000100\n",
      "Epoch [330/500]: Train loss: 1.7260, Valid loss: 1.6826\n",
      "Epoch 330, 当前学习率: 0.000100\n",
      "Epoch [331/500]: Train loss: 1.7256, Valid loss: 1.6759\n",
      "Epoch 331, 当前学习率: 0.000100\n",
      "Epoch [332/500]: Train loss: 1.7257, Valid loss: 1.6822\n",
      "Epoch 332, 当前学习率: 0.000100\n",
      "Epoch [333/500]: Train loss: 1.7259, Valid loss: 1.6770\n",
      "Epoch 333, 当前学习率: 0.000100\n",
      "Epoch [334/500]: Train loss: 1.7265, Valid loss: 1.6836\n",
      "Epoch 334, 当前学习率: 0.000100\n",
      "Epoch [335/500]: Train loss: 1.7263, Valid loss: 1.6813\n",
      "Epoch 335, 当前学习率: 0.000100\n",
      "Epoch [336/500]: Train loss: 1.7257, Valid loss: 1.6868\n",
      "Epoch 336, 当前学习率: 0.000100\n",
      "Epoch [337/500]: Train loss: 1.7260, Valid loss: 1.6841\n",
      "Epoch 337, 当前学习率: 0.000100\n",
      "Epoch [338/500]: Train loss: 1.7258, Valid loss: 1.6809\n",
      "Epoch 338, 当前学习率: 0.000100\n",
      "Epoch [339/500]: Train loss: 1.7260, Valid loss: 1.6647\n",
      "Epoch 339, 当前学习率: 0.000100\n",
      "Epoch [340/500]: Train loss: 1.7260, Valid loss: 1.6730\n",
      "Epoch 340, 当前学习率: 0.000100\n",
      "Epoch [341/500]: Train loss: 1.7263, Valid loss: 1.6843\n",
      "Epoch 341, 当前学习率: 0.000100\n",
      "Epoch [342/500]: Train loss: 1.7259, Valid loss: 1.6824\n",
      "Epoch 342, 当前学习率: 0.000100\n",
      "Epoch [343/500]: Train loss: 1.7260, Valid loss: 1.6730\n",
      "Epoch 343, 当前学习率: 0.000100\n",
      "Epoch [344/500]: Train loss: 1.7261, Valid loss: 1.6889\n",
      "Epoch 344, 当前学习率: 0.000100\n",
      "Epoch [345/500]: Train loss: 1.7266, Valid loss: 1.6675\n",
      "Epoch 345, 当前学习率: 0.000100\n",
      "Epoch [346/500]: Train loss: 1.7261, Valid loss: 1.6700\n",
      "Epoch 346, 当前学习率: 0.000100\n",
      "Epoch [347/500]: Train loss: 1.7259, Valid loss: 1.6719\n",
      "Epoch 347, 当前学习率: 0.000100\n",
      "Epoch [348/500]: Train loss: 1.7264, Valid loss: 1.6873\n",
      "Epoch 348, 当前学习率: 0.000100\n",
      "Epoch [349/500]: Train loss: 1.7258, Valid loss: 1.6678\n",
      "Epoch 349, 当前学习率: 0.000100\n",
      "Epoch [350/500]: Train loss: 1.7263, Valid loss: 1.6724\n",
      "Epoch 350, 当前学习率: 0.000100\n",
      "Epoch [351/500]: Train loss: 1.7257, Valid loss: 1.6756\n",
      "Epoch 351, 当前学习率: 0.000100\n",
      "Epoch [352/500]: Train loss: 1.7260, Valid loss: 1.6677\n",
      "Epoch 352, 当前学习率: 0.000100\n",
      "Epoch [353/500]: Train loss: 1.7258, Valid loss: 1.6788\n",
      "Epoch 353, 当前学习率: 0.000100\n",
      "Epoch [354/500]: Train loss: 1.7259, Valid loss: 1.6818\n",
      "Epoch 354, 当前学习率: 0.000100\n",
      "Epoch [355/500]: Train loss: 1.7260, Valid loss: 1.6833\n",
      "Epoch 355, 当前学习率: 0.000010\n",
      "Epoch [356/500]: Train loss: 1.7257, Valid loss: 1.6698\n",
      "Epoch 356, 当前学习率: 0.000010\n",
      "Epoch [357/500]: Train loss: 1.7257, Valid loss: 1.6752\n",
      "Epoch 357, 当前学习率: 0.000010\n",
      "Epoch [358/500]: Train loss: 1.7254, Valid loss: 1.6806\n",
      "Epoch 358, 当前学习率: 0.000010\n",
      "Epoch [359/500]: Train loss: 1.7260, Valid loss: 1.6861\n",
      "Epoch 359, 当前学习率: 0.000010\n",
      "Epoch [360/500]: Train loss: 1.7253, Valid loss: 1.6822\n",
      "Epoch 360, 当前学习率: 0.000010\n",
      "Epoch [361/500]: Train loss: 1.7254, Valid loss: 1.6893\n",
      "Epoch 361, 当前学习率: 0.000010\n",
      "Epoch [362/500]: Train loss: 1.7257, Valid loss: 1.6634\n",
      "Epoch 362, 当前学习率: 0.000010\n",
      "Epoch [363/500]: Train loss: 1.7252, Valid loss: 1.6704\n",
      "Epoch 363, 当前学习率: 0.000010\n",
      "Epoch [364/500]: Train loss: 1.7253, Valid loss: 1.6862\n",
      "Epoch 364, 当前学习率: 0.000010\n",
      "Epoch [365/500]: Train loss: 1.7255, Valid loss: 1.6828\n",
      "Epoch 365, 当前学习率: 0.000010\n",
      "Epoch [366/500]: Train loss: 1.7254, Valid loss: 1.6731\n",
      "Epoch 366, 当前学习率: 0.000010\n",
      "Epoch [367/500]: Train loss: 1.7252, Valid loss: 1.6712\n",
      "Epoch 367, 当前学习率: 0.000010\n",
      "Epoch [368/500]: Train loss: 1.7259, Valid loss: 1.6693\n",
      "Epoch 368, 当前学习率: 0.000010\n",
      "Epoch [369/500]: Train loss: 1.7253, Valid loss: 1.6794\n",
      "Epoch 369, 当前学习率: 0.000010\n",
      "Epoch [370/500]: Train loss: 1.7252, Valid loss: 1.6744\n",
      "Epoch 370, 当前学习率: 0.000010\n",
      "Epoch [371/500]: Train loss: 1.7251, Valid loss: 1.6809\n",
      "Epoch 371, 当前学习率: 0.000010\n",
      "Epoch [372/500]: Train loss: 1.7253, Valid loss: 1.6725\n",
      "Epoch 372, 当前学习率: 0.000010\n",
      "Epoch [373/500]: Train loss: 1.7254, Valid loss: 1.6736\n",
      "Epoch 373, 当前学习率: 0.000010\n",
      "Epoch [374/500]: Train loss: 1.7258, Valid loss: 1.6804\n",
      "Epoch 374, 当前学习率: 0.000010\n",
      "Epoch [375/500]: Train loss: 1.7257, Valid loss: 1.6676\n",
      "Epoch 375, 当前学习率: 0.000010\n",
      "Epoch [376/500]: Train loss: 1.7256, Valid loss: 1.6845\n",
      "Epoch 376, 当前学习率: 0.000010\n",
      "Epoch [377/500]: Train loss: 1.7258, Valid loss: 1.6770\n",
      "Epoch 377, 当前学习率: 0.000010\n",
      "Epoch [378/500]: Train loss: 1.7253, Valid loss: 1.6832\n",
      "Epoch 378, 当前学习率: 0.000010\n",
      "Epoch [379/500]: Train loss: 1.7255, Valid loss: 1.6830\n",
      "Epoch 379, 当前学习率: 0.000010\n",
      "Epoch [380/500]: Train loss: 1.7256, Valid loss: 1.6644\n",
      "Epoch 380, 当前学习率: 0.000010\n",
      "Epoch [381/500]: Train loss: 1.7257, Valid loss: 1.6817\n",
      "Epoch 381, 当前学习率: 0.000010\n",
      "Epoch [382/500]: Train loss: 1.7255, Valid loss: 1.6734\n",
      "Epoch 382, 当前学习率: 0.000010\n",
      "Epoch [383/500]: Train loss: 1.7256, Valid loss: 1.6769\n",
      "Epoch 383, 当前学习率: 0.000010\n",
      "Epoch [384/500]: Train loss: 1.7256, Valid loss: 1.6817\n",
      "Epoch 384, 当前学习率: 0.000010\n",
      "Epoch [385/500]: Train loss: 1.7254, Valid loss: 1.6739\n",
      "Epoch 385, 当前学习率: 0.000010\n",
      "Epoch [386/500]: Train loss: 1.7260, Valid loss: 1.6711\n",
      "Epoch 386, 当前学习率: 0.000010\n",
      "Epoch [387/500]: Train loss: 1.7256, Valid loss: 1.6816\n",
      "Epoch 387, 当前学习率: 0.000010\n",
      "Epoch [388/500]: Train loss: 1.7258, Valid loss: 1.6816\n",
      "Epoch 388, 当前学习率: 0.000010\n",
      "Epoch [389/500]: Train loss: 1.7258, Valid loss: 1.6714\n",
      "Epoch 389, 当前学习率: 0.000010\n",
      "Epoch [390/500]: Train loss: 1.7254, Valid loss: 1.6865\n",
      "Epoch 390, 当前学习率: 0.000010\n",
      "Epoch [391/500]: Train loss: 1.7253, Valid loss: 1.6805\n",
      "Epoch 391, 当前学习率: 0.000010\n",
      "Epoch [392/500]: Train loss: 1.7258, Valid loss: 1.6766\n",
      "Epoch 392, 当前学习率: 0.000010\n",
      "Epoch [393/500]: Train loss: 1.7254, Valid loss: 1.6749\n",
      "Epoch 393, 当前学习率: 0.000010\n",
      "Epoch [394/500]: Train loss: 1.7253, Valid loss: 1.6817\n",
      "Epoch 394, 当前学习率: 0.000010\n",
      "Epoch [395/500]: Train loss: 1.7256, Valid loss: 1.6891\n",
      "Epoch 395, 当前学习率: 0.000010\n",
      "Epoch [396/500]: Train loss: 1.7257, Valid loss: 1.6775\n",
      "Epoch 396, 当前学习率: 0.000010\n",
      "Epoch [397/500]: Train loss: 1.7254, Valid loss: 1.6955\n",
      "Epoch 397, 当前学习率: 0.000010\n",
      "Epoch [398/500]: Train loss: 1.7258, Valid loss: 1.6813\n",
      "Epoch 398, 当前学习率: 0.000010\n",
      "Epoch [399/500]: Train loss: 1.7258, Valid loss: 1.6822\n",
      "Epoch 399, 当前学习率: 0.000010\n",
      "Epoch [400/500]: Train loss: 1.7259, Valid loss: 1.6824\n",
      "Epoch 400, 当前学习率: 0.000010\n",
      "Epoch [401/500]: Train loss: 1.7260, Valid loss: 1.6780\n",
      "Epoch 401, 当前学习率: 0.000010\n",
      "Epoch [402/500]: Train loss: 1.7254, Valid loss: 1.6722\n",
      "Epoch 402, 当前学习率: 0.000010\n",
      "Epoch [403/500]: Train loss: 1.7257, Valid loss: 1.6695\n",
      "Epoch 403, 当前学习率: 0.000010\n",
      "Epoch [404/500]: Train loss: 1.7254, Valid loss: 1.6705\n",
      "Epoch 404, 当前学习率: 0.000010\n",
      "Epoch [405/500]: Train loss: 1.7256, Valid loss: 1.6799\n",
      "Epoch 405, 当前学习率: 0.000010\n",
      "Epoch [406/500]: Train loss: 1.7253, Valid loss: 1.6754\n",
      "Epoch 406, 当前学习率: 0.000010\n",
      "Epoch [407/500]: Train loss: 1.7255, Valid loss: 1.6700\n",
      "Epoch 407, 当前学习率: 0.000010\n",
      "Epoch [408/500]: Train loss: 1.7255, Valid loss: 1.6721\n",
      "Epoch 408, 当前学习率: 0.000010\n",
      "Epoch [409/500]: Train loss: 1.7257, Valid loss: 1.6875\n",
      "Epoch 409, 当前学习率: 0.000010\n",
      "Epoch [410/500]: Train loss: 1.7256, Valid loss: 1.6756\n",
      "Epoch 410, 当前学习率: 0.000010\n",
      "Epoch [411/500]: Train loss: 1.7254, Valid loss: 1.6683\n",
      "Epoch 411, 当前学习率: 0.000010\n",
      "Epoch [412/500]: Train loss: 1.7254, Valid loss: 1.6947\n",
      "Epoch 412, 当前学习率: 0.000010\n",
      "Epoch [413/500]: Train loss: 1.7258, Valid loss: 1.6772\n",
      "Epoch 413, 当前学习率: 0.000010\n",
      "Epoch [414/500]: Train loss: 1.7256, Valid loss: 1.6878\n",
      "Epoch 414, 当前学习率: 0.000010\n",
      "Epoch [415/500]: Train loss: 1.7260, Valid loss: 1.6877\n",
      "Epoch 415, 当前学习率: 0.000010\n",
      "Epoch [416/500]: Train loss: 1.7259, Valid loss: 1.6711\n",
      "Epoch 416, 当前学习率: 0.000010\n",
      "Epoch [417/500]: Train loss: 1.7258, Valid loss: 1.6865\n",
      "Epoch 417, 当前学习率: 0.000010\n",
      "Epoch [418/500]: Train loss: 1.7253, Valid loss: 1.6688\n",
      "Epoch 418, 当前学习率: 0.000010\n",
      "Epoch [419/500]: Train loss: 1.7257, Valid loss: 1.6861\n",
      "Epoch 419, 当前学习率: 0.000010\n",
      "Epoch [420/500]: Train loss: 1.7254, Valid loss: 1.6651\n",
      "Epoch 420, 当前学习率: 0.000010\n",
      "Epoch [421/500]: Train loss: 1.7257, Valid loss: 1.6630\n",
      "Epoch 421, 当前学习率: 0.000010\n",
      "Epoch [422/500]: Train loss: 1.7256, Valid loss: 1.6800\n",
      "Epoch 422, 当前学习率: 0.000010\n",
      "Epoch [423/500]: Train loss: 1.7257, Valid loss: 1.6861\n",
      "Epoch 423, 当前学习率: 0.000010\n",
      "Epoch [424/500]: Train loss: 1.7256, Valid loss: 1.6707\n",
      "Epoch 424, 当前学习率: 0.000010\n",
      "Epoch [425/500]: Train loss: 1.7255, Valid loss: 1.6787\n",
      "Epoch 425, 当前学习率: 0.000010\n",
      "Epoch [426/500]: Train loss: 1.7256, Valid loss: 1.6849\n",
      "Epoch 426, 当前学习率: 0.000010\n",
      "Epoch [427/500]: Train loss: 1.7255, Valid loss: 1.6764\n",
      "Epoch 427, 当前学习率: 0.000010\n",
      "Epoch [428/500]: Train loss: 1.7259, Valid loss: 1.6735\n",
      "Epoch 428, 当前学习率: 0.000010\n",
      "Epoch [429/500]: Train loss: 1.7255, Valid loss: 1.6709\n",
      "Epoch 429, 当前学习率: 0.000010\n",
      "Epoch [430/500]: Train loss: 1.7257, Valid loss: 1.6882\n",
      "Epoch 430, 当前学习率: 0.000010\n",
      "Epoch [431/500]: Train loss: 1.7256, Valid loss: 1.6748\n",
      "Epoch 431, 当前学习率: 0.000010\n",
      "Epoch [432/500]: Train loss: 1.7256, Valid loss: 1.6858\n",
      "Epoch 432, 当前学习率: 0.000010\n",
      "Epoch [433/500]: Train loss: 1.7257, Valid loss: 1.6759\n",
      "Epoch 433, 当前学习率: 0.000010\n",
      "Epoch [434/500]: Train loss: 1.7255, Valid loss: 1.6739\n",
      "Epoch 434, 当前学习率: 0.000010\n",
      "Epoch [435/500]: Train loss: 1.7255, Valid loss: 1.6798\n",
      "Epoch 435, 当前学习率: 0.000010\n",
      "Epoch [436/500]: Train loss: 1.7253, Valid loss: 1.6668\n",
      "Epoch 436, 当前学习率: 0.000010\n",
      "Epoch [437/500]: Train loss: 1.7258, Valid loss: 1.6684\n",
      "Epoch 437, 当前学习率: 0.000010\n",
      "Epoch [438/500]: Train loss: 1.7253, Valid loss: 1.6710\n",
      "Epoch 438, 当前学习率: 0.000010\n",
      "Epoch [439/500]: Train loss: 1.7257, Valid loss: 1.6820\n",
      "Epoch 439, 当前学习率: 0.000010\n",
      "Epoch [440/500]: Train loss: 1.7258, Valid loss: 1.6757\n",
      "Epoch 440, 当前学习率: 0.000010\n",
      "Epoch [441/500]: Train loss: 1.7252, Valid loss: 1.6716\n",
      "Epoch 441, 当前学习率: 0.000010\n",
      "Epoch [442/500]: Train loss: 1.7255, Valid loss: 1.6723\n",
      "Epoch 442, 当前学习率: 0.000010\n",
      "Epoch [443/500]: Train loss: 1.7259, Valid loss: 1.6842\n",
      "Epoch 443, 当前学习率: 0.000010\n",
      "Epoch [444/500]: Train loss: 1.7253, Valid loss: 1.6761\n",
      "Epoch 444, 当前学习率: 0.000010\n",
      "Epoch [445/500]: Train loss: 1.7257, Valid loss: 1.6765\n",
      "Epoch 445, 当前学习率: 0.000010\n",
      "Epoch [446/500]: Train loss: 1.7251, Valid loss: 1.6684\n",
      "Epoch 446, 当前学习率: 0.000010\n",
      "Epoch [447/500]: Train loss: 1.7255, Valid loss: 1.6853\n",
      "Epoch 447, 当前学习率: 0.000010\n",
      "Epoch [448/500]: Train loss: 1.7256, Valid loss: 1.6854\n",
      "Epoch 448, 当前学习率: 0.000010\n",
      "Epoch [449/500]: Train loss: 1.7253, Valid loss: 1.6701\n",
      "Epoch 449, 当前学习率: 0.000010\n",
      "Epoch [450/500]: Train loss: 1.7254, Valid loss: 1.6657\n",
      "Epoch 450, 当前学习率: 0.000010\n",
      "Epoch [451/500]: Train loss: 1.7255, Valid loss: 1.6776\n",
      "Epoch 451, 当前学习率: 0.000010\n",
      "Epoch [452/500]: Train loss: 1.7254, Valid loss: 1.6747\n",
      "Epoch 452, 当前学习率: 0.000010\n",
      "Epoch [453/500]: Train loss: 1.7257, Valid loss: 1.6612\n",
      "Epoch 453, 当前学习率: 0.000010\n",
      "Epoch [454/500]: Train loss: 1.7256, Valid loss: 1.6744\n",
      "Epoch 454, 当前学习率: 0.000010\n",
      "Epoch [455/500]: Train loss: 1.7257, Valid loss: 1.6728\n",
      "Epoch 455, 当前学习率: 0.000010\n",
      "Epoch [456/500]: Train loss: 1.7255, Valid loss: 1.6694\n",
      "Epoch 456, 当前学习率: 0.000010\n",
      "Epoch [457/500]: Train loss: 1.7255, Valid loss: 1.6808\n",
      "Epoch 457, 当前学习率: 0.000010\n",
      "Epoch [458/500]: Train loss: 1.7258, Valid loss: 1.6778\n",
      "Epoch 458, 当前学习率: 0.000010\n",
      "Epoch [459/500]: Train loss: 1.7255, Valid loss: 1.6709\n",
      "Epoch 459, 当前学习率: 0.000010\n",
      "Epoch [460/500]: Train loss: 1.7257, Valid loss: 1.6714\n",
      "Epoch 460, 当前学习率: 0.000010\n",
      "Epoch [461/500]: Train loss: 1.7255, Valid loss: 1.6787\n",
      "Epoch 461, 当前学习率: 0.000010\n",
      "Epoch [462/500]: Train loss: 1.7250, Valid loss: 1.6769\n",
      "Epoch 462, 当前学习率: 0.000010\n",
      "Epoch [463/500]: Train loss: 1.7256, Valid loss: 1.6830\n",
      "Epoch 463, 当前学习率: 0.000010\n",
      "Epoch [464/500]: Train loss: 1.7252, Valid loss: 1.6757\n",
      "Epoch 464, 当前学习率: 0.000010\n",
      "Epoch [465/500]: Train loss: 1.7257, Valid loss: 1.6799\n",
      "Epoch 465, 当前学习率: 0.000010\n",
      "Epoch [466/500]: Train loss: 1.7256, Valid loss: 1.6850\n",
      "Epoch 466, 当前学习率: 0.000010\n",
      "Epoch [467/500]: Train loss: 1.7254, Valid loss: 1.6795\n",
      "Epoch 467, 当前学习率: 0.000010\n",
      "Epoch [468/500]: Train loss: 1.7258, Valid loss: 1.6792\n",
      "Epoch 468, 当前学习率: 0.000010\n",
      "Epoch [469/500]: Train loss: 1.7258, Valid loss: 1.6835\n",
      "Epoch 469, 当前学习率: 0.000010\n",
      "Epoch [470/500]: Train loss: 1.7257, Valid loss: 1.6819\n",
      "Epoch 470, 当前学习率: 0.000010\n",
      "Epoch [471/500]: Train loss: 1.7256, Valid loss: 1.6805\n",
      "Epoch 471, 当前学习率: 0.000010\n",
      "Epoch [472/500]: Train loss: 1.7255, Valid loss: 1.6846\n",
      "Epoch 472, 当前学习率: 0.000010\n",
      "Epoch [473/500]: Train loss: 1.7252, Valid loss: 1.6874\n",
      "Epoch 473, 当前学习率: 0.000010\n",
      "Epoch [474/500]: Train loss: 1.7256, Valid loss: 1.6692\n",
      "Epoch 474, 当前学习率: 0.000010\n",
      "Epoch [475/500]: Train loss: 1.7258, Valid loss: 1.6724\n",
      "Epoch 475, 当前学习率: 0.000010\n",
      "Epoch [476/500]: Train loss: 1.7253, Valid loss: 1.6666\n",
      "Epoch 476, 当前学习率: 0.000010\n",
      "Epoch [477/500]: Train loss: 1.7257, Valid loss: 1.6792\n",
      "Epoch 477, 当前学习率: 0.000010\n",
      "Epoch [478/500]: Train loss: 1.7259, Valid loss: 1.6729\n",
      "Epoch 478, 当前学习率: 0.000010\n",
      "Epoch [479/500]: Train loss: 1.7258, Valid loss: 1.6688\n",
      "Epoch 479, 当前学习率: 0.000010\n",
      "Epoch [480/500]: Train loss: 1.7258, Valid loss: 1.6760\n",
      "Epoch 480, 当前学习率: 0.000010\n",
      "Epoch [481/500]: Train loss: 1.7254, Valid loss: 1.6799\n",
      "Epoch 481, 当前学习率: 0.000010\n",
      "Epoch [482/500]: Train loss: 1.7256, Valid loss: 1.6756\n",
      "Epoch 482, 当前学习率: 0.000010\n",
      "Epoch [483/500]: Train loss: 1.7254, Valid loss: 1.6775\n",
      "Epoch 483, 当前学习率: 0.000010\n",
      "Epoch [484/500]: Train loss: 1.7254, Valid loss: 1.6733\n",
      "Epoch 484, 当前学习率: 0.000010\n",
      "Epoch [485/500]: Train loss: 1.7256, Valid loss: 1.6736\n",
      "Epoch 485, 当前学习率: 0.000010\n",
      "Epoch [486/500]: Train loss: 1.7257, Valid loss: 1.6847\n",
      "Epoch 486, 当前学习率: 0.000010\n",
      "Epoch [487/500]: Train loss: 1.7254, Valid loss: 1.6900\n",
      "Epoch 487, 当前学习率: 0.000010\n",
      "Epoch [488/500]: Train loss: 1.7256, Valid loss: 1.6848\n",
      "Epoch 488, 当前学习率: 0.000010\n",
      "Epoch [489/500]: Train loss: 1.7258, Valid loss: 1.6788\n",
      "Epoch 489, 当前学习率: 0.000010\n",
      "Epoch [490/500]: Train loss: 1.7254, Valid loss: 1.6892\n",
      "Epoch 490, 当前学习率: 0.000010\n",
      "Epoch [491/500]: Train loss: 1.7257, Valid loss: 1.6772\n",
      "Epoch 491, 当前学习率: 0.000010\n",
      "Epoch [492/500]: Train loss: 1.7256, Valid loss: 1.6763\n",
      "Epoch 492, 当前学习率: 0.000010\n",
      "Epoch [493/500]: Train loss: 1.7256, Valid loss: 1.6730\n",
      "Epoch 493, 当前学习率: 0.000010\n",
      "Epoch [494/500]: Train loss: 1.7252, Valid loss: 1.6766\n",
      "Epoch 494, 当前学习率: 0.000010\n",
      "Epoch [495/500]: Train loss: 1.7257, Valid loss: 1.6924\n",
      "Epoch 495, 当前学习率: 0.000010\n",
      "Epoch [496/500]: Train loss: 1.7257, Valid loss: 1.6746\n",
      "Epoch 496, 当前学习率: 0.000010\n",
      "Epoch [497/500]: Train loss: 1.7255, Valid loss: 1.6735\n",
      "Epoch 497, 当前学习率: 0.000010\n",
      "Epoch [498/500]: Train loss: 1.7257, Valid loss: 1.6667\n",
      "Epoch 498, 当前学习率: 0.000010\n",
      "Epoch [499/500]: Train loss: 1.7259, Valid loss: 1.6705\n",
      "Epoch 499, 当前学习率: 0.000010\n",
      "Epoch [500/500]: Train loss: 1.7255, Valid loss: 1.6762\n",
      "Epoch 500, 当前学习率: 0.000010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 299.91it/s]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# Reading/Writing Data\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "\n",
    "# For Progress Bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Pytorch\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "# For plotting learning curve\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "\n",
    "# %%\n",
    "def same_seed(seed): \n",
    "    '''Fixes random number generator seeds for reproducibility.'''\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def train_valid_split(data_set, valid_ratio, seed):\n",
    "    '''Split provided training data into training set and validation set'''\n",
    "    valid_set_size = int(valid_ratio * len(data_set)) \n",
    "    train_set_size = len(data_set) - valid_set_size\n",
    "    train_set, valid_set = random_split(data_set, [train_set_size, valid_set_size], generator=torch.Generator().manual_seed(seed))\n",
    "    return np.array(train_set), np.array(valid_set)\n",
    "\n",
    "def predict(test_loader, model, device):\n",
    "    model.eval() # Set your model to evaluation mode.\n",
    "    preds = []\n",
    "    for x in tqdm(test_loader):\n",
    "        x = x.to(device)                        \n",
    "        with torch.no_grad():                   \n",
    "            pred = model(x)                     \n",
    "            preds.append(pred.detach().cpu())   \n",
    "    preds = torch.cat(preds, dim=0).numpy()  \n",
    "    return preds\n",
    "\n",
    "# %% [markdown]\n",
    "# # Dataset\n",
    "\n",
    "# %%\n",
    "class myDataset(Dataset):\n",
    "    '''\n",
    "    x: Features.\n",
    "    y: Targets, if none, do prediction.\n",
    "    '''\n",
    "    def __init__(self, x, y=None):\n",
    "        if y is None:\n",
    "            self.y = y\n",
    "        else:\n",
    "            self.y = torch.FloatTensor(y)\n",
    "        self.x = torch.FloatTensor(x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.x[idx]\n",
    "        if len(sample.shape) == 2:  \n",
    "            sample = sample.unsqueeze(0)  # Add channel dimension for CNN input\n",
    "        if self.y is None:\n",
    "            return sample\n",
    "        else:\n",
    "            return sample, self.y[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "# %% [markdown]\n",
    "# # Neural Network Model\n",
    "# Try out different model architectures by modifying the class below.\n",
    "\n",
    "# %%\n",
    "class image2sequence(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim):\n",
    "        super(image2sequence, self).__init__()\n",
    "        #input : 1*16*16\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 5, 1, 2), \n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2), #88\n",
    "            \n",
    "            nn.Conv2d(32, 64, 5, 1, 2),\n",
    "            nn.BatchNorm2d(64), \n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2), #44\n",
    "\n",
    "            \n",
    "            nn.Conv2d(64, 64, 3, 1, 1),\n",
    "            nn.BatchNorm2d(64), \n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2) #22\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64*2*2, 256),\n",
    "            # nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            # nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        x = x.squeeze(1) # (B, 1) -> (B)\n",
    "        return x\n",
    "\n",
    "class sequence2sequence(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim, dropout=0.5):\n",
    "        super(sequence2sequence, self).__init__()\n",
    "        # TODO: modify model's structure, be aware of dimensions. \n",
    "        neuron_num = [input_dim] + hidden_dim + [output_dim]\n",
    "        layers = []\n",
    "        for i in range(1, len(neuron_num)):\n",
    "            layers.append(nn.Linear(neuron_num[i-1], neuron_num[i]))\n",
    "            if i < len(neuron_num) - 1:\n",
    "                layers.append(nn.BatchNorm1d(neuron_num[i]))\n",
    "                layers.append(nn.ReLU())\n",
    "                layers.append(nn.Dropout(dropout))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        x = x.squeeze(1) # (B, 1) -> (B)\n",
    "        return x\n",
    "\n",
    "class MLPS(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim, dropout=0.5):\n",
    "        super(MLPS, self).__init__()\n",
    "        # TODO: modify model's structure, be aware of dimensions. \n",
    "        self.num_phi_slices = output_dim[1]\n",
    "        self.theta_dim = output_dim[0]\n",
    "\n",
    "        self.mlps = nn.ModuleList()\n",
    "        neuron_num = [input_dim] + hidden_dim + [self.theta_dim]\n",
    "\n",
    "        for _ in range(self.num_phi_slices):\n",
    "            layers = []\n",
    "            for j in range(1, len(neuron_num)):\n",
    "                layers.append(nn.Linear(neuron_num[j-1], neuron_num[j]))\n",
    "                if j < len(neuron_num) - 1:\n",
    "                    layers.append(nn.BatchNorm1d(neuron_num[j]))\n",
    "                    layers.append(nn.ReLU())\n",
    "                    layers.append(nn.Dropout(dropout))\n",
    "            self.layers = nn.Sequential(*layers)\n",
    "            self.mlps.append(self.layers)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        output = list()\n",
    "        for i in range(self.num_phi_slices):\n",
    "            output.append(self.mlps[i](x))\n",
    "        x = torch.stack(output, dim=2)  \n",
    "        return x\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_dim=100, ngf=64, output_dim=3):\n",
    "        \"\"\"\n",
    "        DCGAN 生成器\n",
    "        \n",
    "        参数:\n",
    "        - nz (int): 潜入向量z的维度\n",
    "        - ngf (int): 生成器特征图的基础深度\n",
    "        - nc (int): 输出图像的通道数\n",
    "        \"\"\"\n",
    "        super(Generator, self).__init__()\n",
    "        self.ngf = ngf  # 基础特征图深度\n",
    "        output_dim = output_dim[0] # [C,H,W]\n",
    "        # -----------------------------------------------------\n",
    "        # 步骤 1: Project and Reshape\n",
    "        # 目标: 将 (B, nz) 的噪声向量转换为 (B, ngf * 8, 4, 4) 的初始特征图\n",
    "        # B = batch_size\n",
    "        # -----------------------------------------------------\n",
    "        self.project_and_reshape = nn.Sequential(\n",
    "            # \"Project\": 全连接层将 nz 维向量投射到高维空间\n",
    "            # 4*4 是初始特征图的空间尺寸，ngf*8 是通道数\n",
    "            nn.Linear(input_dim, ngf * 8 * 4 * 4), \n",
    "            \n",
    "            # 批量归一化，稳定学习\n",
    "            nn.BatchNorm1d(ngf * 8 * 4 * 4),\n",
    "            \n",
    "            # 使用 LeakyReLU 激活函数，引入非线性\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        \n",
    "        # -----------------------------------------------------\n",
    "        # 步骤 2: 主体部分，使用转置卷积 (Transposed Convolution) 逐步放大\n",
    "        # -----------------------------------------------------\n",
    "        self.main = nn.Sequential(\n",
    "            # 输入: (B, ngf * 8, 4, 4)\n",
    "            # 输出: (B, ngf * 4, 8, 8)\n",
    "            nn.ConvTranspose2d(in_channels=ngf * 8, out_channels=ngf * 4, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # 输入: (B, ngf * 4, 8, 8)\n",
    "            # 输出: (B, ngf * 2, 16, 16)\n",
    "            nn.ConvTranspose2d(in_channels=ngf * 4, out_channels=ngf * 2, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # 输入: (B, ngf * 2, 16, 16)\n",
    "            # 输出: (B, ngf, 32, 32)\n",
    "            nn.ConvTranspose2d(in_channels=ngf * 2, out_channels=ngf, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # 输入: (B, ngf, 32, 32)\n",
    "            # 输出: (B, nc, 64, 64)\n",
    "            nn.ConvTranspose2d(in_channels=ngf, out_channels=output_dim, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            \n",
    "            # 输出层使用 Tanh 激活函数，将输出归一化到 [-1, 1] 范围\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        前向传播\n",
    "        \n",
    "        参数:\n",
    "        - input (Tensor): 输入的噪声向量，形状为 (B, nz)\n",
    "        \"\"\"\n",
    "        # 1. Project\n",
    "        # 输入: (B, nz) -> 输出: (B, ngf * 8 * 4 * 4)\n",
    "        x = self.project_and_reshape(input)\n",
    "        \n",
    "        ngf = self.ngf\n",
    "        # 2. Reshape\n",
    "        # 将一维向量重塑为三维张量，以匹配卷积层的输入要求\n",
    "        # (B, ngf * 8 * 4 * 4) -> (B, ngf * 8, 4, 4)\n",
    "        x = x.view(-1, ngf * 8, 4, 4) # -1 表示自动计算 batch_size\n",
    "        \n",
    "        # 3. 通过转置卷积层生成图像\n",
    "        # (B, ngf * 8, 4, 4) -> (B, nc, 64, 64)\n",
    "        output = self.main(x)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# %%\n",
    "def select_feat(train_data, valid_data, test_data, input_dim, output_dim, select_all=True):\n",
    "    '''Selects useful features to perform regression'''\n",
    "    YTrain, YValidation, y_test = train_data[:,input_dim:], valid_data[:,input_dim:], test_data[:,input_dim:]\n",
    "    raw_x_train, raw_XValidation, raw_x_test = train_data[:,:input_dim], valid_data[:,:input_dim], test_data[:,:input_dim]\n",
    "\n",
    "    if select_all:\n",
    "        feat_idx = list(range(raw_x_train.shape[1]))\n",
    "    else:\n",
    "        feat_idx = [0,1,2,3,4] # TODO: Select suitable feature columns.\n",
    "        \n",
    "    return raw_x_train[:,feat_idx], raw_XValidation[:,feat_idx], raw_x_test[:,feat_idx], YTrain, YValidation, y_test\n",
    "\n",
    "# %% [markdown]\n",
    "# # Training Loop\n",
    "\n",
    "# %%\n",
    "def trainer(train_loader, valid_loader, model, config, device):\n",
    "\n",
    "    criterion = nn.MSELoss(reduction='mean') # Define your loss function, do not modify this.\n",
    "\n",
    "    # Define your optimization algorithm. \n",
    "    # TODO: Please check https://pytorch.org/docs/stable/optim.html to get more available algorithms.\n",
    "    # TODO: L2 regularization (optimizer(weight decay...) or implement by your self).\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate']) \n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=100) # Learning rate scheduler.\n",
    "    writer = SummaryWriter() # Writer of tensoboard.\n",
    "\n",
    "    if not os.path.isdir('./models'):\n",
    "        os.mkdir('./models') # Create directory of saving models.\n",
    "\n",
    "    n_epochs, best_loss, step, early_stop_count = config['n_epochs'], math.inf, 0, 0\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train() # Set your model to train mode.\n",
    "        loss_record = []\n",
    "\n",
    "        # tqdm is a package to visualize your training progress.\n",
    "        # train_pbar = tqdm(train_loader, position=0, leave=True)\n",
    "\n",
    "        for x, y in train_loader:\n",
    "            optimizer.zero_grad()               # Set gradient to zero.\n",
    "            x, y = x.to(device), y.to(device)   # Move your data to device. \n",
    "            pred = model(x)             \n",
    "            loss = criterion(pred, y)\n",
    "            loss.backward()                     # Compute gradient(backpropagation).\n",
    "            optimizer.step()                    # Update parameters.\n",
    "            step += 1\n",
    "            loss_record.append(loss.detach().item())\n",
    "            \n",
    "            # Display current epoch number and loss on tqdm progress bar.\n",
    "            # train_pbar.set_description(f'Epoch [{epoch+1}/{n_epochs}]')\n",
    "            # train_pbar.set_postfix({'loss': loss.detach().item()})\n",
    "\n",
    "        mean_train_loss = sum(loss_record)/len(loss_record)\n",
    "        writer.add_scalar('Loss/train', mean_train_loss, step)\n",
    "\n",
    "        model.eval() # Set your model to evaluation mode.\n",
    "        loss_record = []\n",
    "        for x, y in valid_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            with torch.no_grad():\n",
    "                pred = model(x)\n",
    "                loss = criterion(pred, y)\n",
    "\n",
    "            loss_record.append(loss.item())\n",
    "            \n",
    "        mean_valid_loss = sum(loss_record)/len(loss_record)\n",
    "        print(f'Epoch [{epoch+1}/{n_epochs}]: Train loss: {mean_train_loss:.4f}, Valid loss: {mean_valid_loss:.4f}')\n",
    "        writer.add_scalar('Loss/valid', mean_valid_loss, step)\n",
    "        scheduler.step(mean_valid_loss) # Update learning rate.\n",
    "            # 打印当前学习率\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f\"Epoch {epoch+1}, 当前学习率: {current_lr:.6f}\")\n",
    "\n",
    "        if mean_valid_loss < best_loss:\n",
    "            best_loss = mean_valid_loss\n",
    "            # torch.save(model.state_dict(), config['save_path']) # Save your best model\n",
    "            # print('Saving model with loss {:.3f}...'.format(best_loss))\n",
    "            early_stop_count = 0\n",
    "        else: \n",
    "            early_stop_count += 1\n",
    "\n",
    "        if early_stop_count >= config['early_stop']:\n",
    "            print('\\nModel is not improving, so we halt the training session. best_loss: {:.3f}'.format(best_loss))\n",
    "            return\n",
    "\n",
    "def standerize2d(data, mean, std):\n",
    "    mean = data.mean(dim=0)\n",
    "    std = data.std(dim=0)\n",
    "    return (data - mean) / std\n",
    "\n",
    "def unstanderize2d(data, mean, std):\n",
    "    return data * std + mean\n",
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "config = {\n",
    "    'seed': 3407,      # Your seed number, you can pick your lucky number. :)\n",
    "    'select_all': True,   # Whether to use all features.\n",
    "    'valid_ratio': 0.1,   # validation_size = train_size * valid_ratio\n",
    "    'n_epochs': 500,     # Number of epochs.            \n",
    "    'batch_size': 256, \n",
    "    'learning_rate': 1e-3,              \n",
    "    'early_stop': 400,    # If model has not improved for this many consecutive epochs, stop training.     \n",
    "    'save_path': './models/model.ckpt',  # Your model will be saved here.\n",
    "    'hidden_dim': [200]*5\n",
    "}\n",
    "\n",
    "# %%\n",
    "# Set seed for reproducibility\n",
    "same_seed(config['seed'])\n",
    "\n",
    "mat_contents = scipy.io.loadmat('dataset.mat') # Save predictions to a .mat file.\n",
    "XTrain, XValidation= mat_contents['XTrain'], mat_contents['XValidation']\n",
    "YTrain, YValidation = mat_contents['YTrain'], mat_contents['YValidation']\n",
    "\n",
    "if len(YTrain.shape) == 2:  # If data is 1D, add a channel dimension.\n",
    "    YTrain = YTrain[:,:,np.newaxis]\n",
    "    YValidation = YValidation[:,:,np.newaxis]\n",
    "print(f\"\"\"\n",
    "XTrain size: {XTrain.shape}\n",
    "train_data size: {YTrain.shape} \n",
    "\"\"\")\n",
    "\n",
    "YTrain, YValidation = torch.FloatTensor(YTrain), torch.FloatTensor(YValidation) # Convert targets to torch tensors.\n",
    "\n",
    "YTrain_main = YTrain.mean(dim=0) # Calculate mean of targets for standardization.\n",
    "YValidation_main = YValidation.mean(dim=0) # Calculate mean of targets for standardization.\n",
    "YTrain_std = YTrain.std(dim=0) # Calculate std of targets for standardization.\n",
    "YValidation_std = YValidation.std(dim=0) # Calculate std of targets for standardization.\n",
    "YTrain_standerize = standerize2d(YTrain, YTrain_main, YTrain_std) # Standardize targets.\n",
    "YValidation_standerize = standerize2d(YValidation, YValidation_main, YValidation_std) # Standardize targets.\n",
    "\n",
    "train_dataset, valid_dataset, test_dataset = myDataset(XTrain, YTrain_standerize), \\\n",
    "                                            myDataset(XValidation, YValidation_standerize),\\\n",
    "                                            myDataset(XValidation) # Create datasets.\n",
    "\n",
    "\n",
    "# Pytorch data loader loads pytorch dataset into batches.\n",
    "train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True, pin_memory=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=config['batch_size'], shuffle=True, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False, pin_memory=True)\n",
    "\n",
    "input_dim=XTrain.shape[1]\n",
    "output_dim=YTrain.shape[1:]\n",
    "hidden_dim = config['hidden_dim'] \n",
    "print(output_dim)\n",
    "# model = image2sequence(input_dim=XTrain.shape[1], output_dim=YTrain.shape[1], hidden_dim=hidden_dim).to(device) # Initialize your model.\n",
    "# model = sequence2sequence(input_dim=XTrain.shape[1], output_dim=YTrain.shape[1], hidden_dim=hidden_dim, dropout=0.2).to(device)\n",
    "# model = MLPS(input_dim, output_dim, hidden_dim=hidden_dim, dropout=0.2).to(device)\n",
    "model = Generator(input_dim, 64, output_dim).to(device) # Initialize your model.\n",
    "trainer(train_loader, valid_loader, model, config, device) # Train your model.\n",
    "preds_standerize = predict(test_loader, model, device) \n",
    "preds = unstanderize2d(torch.FloatTensor(preds_standerize), YValidation_main, YValidation_std) # Unstandardize predictions.\n",
    "cretirion = nn.MSELoss()\n",
    "loss_value = cretirion(preds, torch.FloatTensor(YValidation)) # Calculate loss of predictions.\n",
    "scipy.io.savemat('preds.mat', {'preds': preds}) # Save predictions to a .mat file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "93533bfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[  0.98049927,   0.28635862,  -0.568267  ,  -1.5861373 ,\n",
       "          -2.3934479 ,  -2.83153   ,  -2.8073442 ,  -2.7200942 ,\n",
       "          -2.4276612 ,  -1.8279647 ],\n",
       "        [  0.8505165 ,  -0.79245216,  -2.4186924 ,  -3.7863274 ,\n",
       "          -4.6688533 ,  -4.8311224 ,  -4.4826045 ,  -3.779292  ,\n",
       "          -2.7397046 ,  -1.301     ],\n",
       "        [  2.3676894 ,   0.10250846,  -2.4308267 ,  -4.7916265 ,\n",
       "          -6.5796366 ,  -7.3681345 ,  -7.0332837 ,  -6.0776854 ,\n",
       "          -4.685646  ,  -2.8620377 ],\n",
       "        [  3.5152595 ,   0.3890637 ,  -2.8400033 ,  -5.818548  ,\n",
       "          -8.040739  ,  -9.237865  ,  -9.21668   ,  -8.11403   ,\n",
       "          -6.2133145 ,  -3.5316    ],\n",
       "        [  3.9668489 ,   0.21133207,  -3.6587555 ,  -6.8692284 ,\n",
       "          -9.265078  , -10.57837   , -10.717793  ,  -9.45407   ,\n",
       "          -7.071605  ,  -3.8641744 ],\n",
       "        [  4.00206   ,   0.2842212 ,  -3.6863625 ,  -7.052696  ,\n",
       "          -9.513619  , -10.757155  , -10.8628025 ,  -9.608764  ,\n",
       "          -7.158276  ,  -3.9224317 ],\n",
       "        [  3.5623846 ,   0.54351825,  -2.7370992 ,  -5.7950706 ,\n",
       "          -8.228957  ,  -9.540902  ,  -9.540673  ,  -8.328013  ,\n",
       "          -6.2738943 ,  -3.5536358 ],\n",
       "        [  2.8194752 ,   0.5967232 ,  -1.8940058 ,  -4.4491024 ,\n",
       "          -6.362774  ,  -7.193314  ,  -6.984299  ,  -5.913367  ,\n",
       "          -4.397766  ,  -2.4978085 ],\n",
       "        [  1.2810276 ,  -0.20457508,  -1.7251987 ,  -3.2027586 ,\n",
       "          -4.2739844 ,  -4.504147  ,  -4.2273827 ,  -3.4720843 ,\n",
       "          -2.4118295 ,  -0.9741098 ],\n",
       "        [  1.8288268 ,   1.0887767 ,   0.35657915,  -0.59468454,\n",
       "          -1.3105955 ,  -1.7681935 ,  -1.8656015 ,  -1.8517877 ,\n",
       "          -1.5805196 ,  -1.1012678 ]]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b80a19ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhYAAAGdCAYAAABO2DpVAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAZu1JREFUeJzt3Xd4VNXWx/HvpPcEAgkEQm8hlABBRRBEEKUJyrXrRbG8Kiq2a7+Wa0Gv5VoQFAvYK4KI9CKIiHQIoYROqKEmIZA68/5xmJAoJZPMzJnJ/D7PM09OJjPnrJCQWbP32mtbbDabDREREREn8DM7ABEREak+lFiIiIiI0yixEBEREadRYiEiIiJOo8RCREREnEaJhYiIiDiNEgsRERFxGiUWIiIi4jQB7r6g1Wplz549REZGYrFY3H15ERERqQSbzUZubi4JCQn4+Z15XMLticWePXtITEx092VFRETECTIzM6lfv/4Zv+72xCIyMhIwAouKinL35UVERKQScnJySExMLH0dPxO3Jxb26Y+oqCglFiIiIl7mXGUMKt4UERERp1FiISIiIk6jxEJEREScxu01FiIi4l42m43i4mJKSkrMDkU8mL+/PwEBAVVuBaHEQkSkGissLGTv3r0cP37c7FDEC4SFhVG3bl2CgoIqfQ4lFiIi1ZTVamXbtm34+/uTkJBAUFCQGhPKadlsNgoLCzlw4ADbtm2jefPmZ22CdTZKLEREqqnCwkKsViuJiYmEhYWZHY54uNDQUAIDA9mxYweFhYWEhIRU6jwq3hQRqeYq+85TfI8zflf02yYiIiJOo8RCREREnEaJhYiI+KznnnuOlJSU0s9vueUWBg8e7PY4tm/fjsViYdWqVW6/trMpsRAREY9zyy23YLFYsFgsBAYG0qRJEx555BHy8vJcet23336b8ePHV+ix1SkZcCatChERcTebDZZ+BA26QJ02ZkfjsS6//HLGjRtHUVERv/32G7fffjt5eXmMGTOm3OOKiooIDAx0yjWjo6Odch5fphELERF3y5gOUx+B7/5pJBluYrPZOF5YbMrNVonvMzg4mDp16pCYmMgNN9zAjTfeyKRJk0qnLz755BOaNGlCcHAwNpuN7Oxs7rzzTuLi4oiKiuKSSy5h9erV5c75yiuvEB8fT2RkJLfddhv5+fnlvv7XqRCr1cqrr75Ks2bNCA4OpkGDBrz00ksANG7cGIAOHTpgsVi4+OKLS583btw4kpKSCAkJoVWrVowePbrcdZYsWUKHDh0ICQkhNTWVlStXOvzv46k0YiEi4m6ZS4yPh7fAnhVQr5NbLnuiqITWz8xwy7X+at1/LiMsqGovOaGhoRQVFQGwefNmvvvuOyZMmIC/vz8A/fv3p2bNmkydOpXo6Gg++OADevXqRUZGBjVr1uS7777j2Wef5b333uOiiy7i888/55133qFJkyZnvOYTTzzBhx9+yP/+9z+6devG3r172bBhA2AkB+eddx6zZ88mOTm5tFvlhx9+yLPPPsuoUaPo0KEDK1eu5I477iA8PJyhQ4eSl5fHgAEDuOSSS/jiiy/Ytm0bI0aMqNK/jSdRYiEi4m57V506TvvBbYmFN1uyZAlfffUVvXr1AozmX59//jm1a9cGYO7cuaSlpZGVlUVwcDAAr7/+OpMmTeKHH37gzjvv5K233mLYsGHcfvvtALz44ovMnj37b6MWdrm5ubz99tuMGjWKoUOHAtC0aVO6desGUHrt2NhY6tSpU/q8F154gTfeeIOrrroKMEY21q1bxwcffMDQoUP58ssvKSkp4ZNPPiEsLIzk5GR27drF3Xff7ex/NlMosRARcSebDfasOvX52gnQ50Xw83f5pUMD/Vn3n8tcfp0zXdtRU6ZMISIiguLiYoqKihg0aBDvvvsuo0ePpmHDhqUv7ADLly/n2LFjxMbGljvHiRMn2LJlCwDr16/nrrvuKvf1Ll26MG/evNNef/369RQUFJQmMxVx4MABMjMzue2227jjjjtK7y8uLi6t31i/fj3t27cv1w21S5cuFb6Gp1NiISLiTkd3wonD4BcAQRFwbD9s/w2aXOzyS1sslipPR7hTz549GTNmDIGBgSQkJJQr0AwPDy/3WKvVSt26dfn111//dp6YmJhKXT80NNTh51itVsCYDjn//PPLfc0+ZVOZehNvouJNERF3sk+DxLWG5MHGcdoPZkXj0cLDw2nWrBkNGzY856qPjh07sm/fPgICAmjWrFm5W61atQBISkpi8eLF5Z7318/Lat68OaGhocyZM+e0X7fXVJTdjj4+Pp569eqxdevWv8VhL/Zs3bo1q1ev5sSJExWKw9sosRARcSf7NEhCCrS92jheNxmKC8yKqFro3bs3Xbp0YfDgwcyYMYPt27ezaNEinn76aZYtWwbAiBEj+OSTT/jkk0/IyMjg2WefJT09/YznDAkJ4bHHHuPRRx/ls88+Y8uWLSxevJiPP/4YgLi4OEJDQ5k+fTr79+8nOzsbMJpujRw5krfffpuMjAzS0tIYN24cb775JgA33HADfn5+3Hbbbaxbt46pU6fy+uuvu/hfyH2UWIiIuNOek8sKEzpAgwshMgEKsmHTLHPj8nIWi4WpU6fSvXt3hg0bRosWLbjuuuvYvn078fHxAFx77bU888wzPPbYY3Tq1IkdO3acs2Dy3//+Nw8//DDPPPMMSUlJXHvttWRlZQEQEBDAO++8wwcffEBCQgKDBg0C4Pbbb+ejjz5i/PjxtG3blh49ejB+/PjSEYuIiAh+/vln1q1bR4cOHXjqqad49dVXXfiv414Wm5sne3JycoiOjiY7O5uoqCh3XlpExFw2G/y3MZw4AnfMg3odYebTsOhdaD0YrvnUqZfLz89n27ZtNG7cuNJbYItvOdvvTEVfvzViISLiLkd3GkmFXyDEJxv3tfmH8TFjOuTnmBebiJMosRARcRd74WZ8awgwei1Qtz3ENofifNg41bTQRJxFiYWIiLvY6yvqppy6z2I5VcSZ9r3bQxJxNiUWIiLuUnZFSFltT06HbJkHxw64MyIRp3MosWjUqFHpNrZlb8OHD3dVfCIi1YPNdmoqpOyIBUBsU2OViK0E1k1yc2AizuVQYrF06VL27t1beps1y1gedfXVV7skOBGRauPojr8XbpZVOh2iZlni3RxKLGrXrk2dOnVKb1OmTKFp06b06NHDVfGJiFQP9mmQsoWbZSVfBVggczEc2eHOyEScqtI1FoWFhXzxxRcMGzYMi8VyxscVFBSQk5NT7iYi4nPONA1iF1UXGl9kHK+d4I6IRFyi0onFpEmTOHr0KLfccstZHzdy5Eiio6NLb4mJiZW9pIiI9yot3Oxw5sdoOsTrPffcc6SkpJgdBgC33HILgwcPdvt1K51YfPzxx/Tt25eEhISzPu6JJ54gOzu79JaZmVnZS4qIeCebrUwr75QzPy5poFGDkZUO+9e5JTRPtW/fPkaMGEGzZs0ICQkhPj6ebt268f7773P8+HGzw6uU55577rQLIMretm/f7vB5t2/fjsViYdWqVU6PuTIqtX/ujh07mD17Nj/++OM5HxscHExw8GnmE0VEfMXRHZB/1Ega4lqf+XGhNaB5H9j4C6z9AeKfcVuInmTr1q107dqVmJgYXn75Zdq2bUtxcTEZGRl88sknJCQkcMUVV5z2uUVFRefcCdUsjzzyCHfddVfp5507d+bOO+/kjjvuKL2vdu3apceFhYWlO6h6k0qNWIwbN464uDj69+/v7HhERKqf0sLN5NMXbpZl72mR9r0x0uGD7rnnHgICAli2bBnXXHMNSUlJtG3bliFDhvDLL78wcODA0sdaLBbef/99Bg0aRHh4OC+++CIAY8aMoWnTpgQFBdGyZUs+//zz0uec7h3+0aNHsVgs/PrrrwD8+uuvWCwW5syZQ2pqKmFhYVx44YVs3LixXKyvvPIK8fHxREZGctttt5Gfn3/G7ysiIqLcAgh/f38iIyNLP3/88ccZMmQII0eOJCEhgRYtWpR+j5MmTSp3rpiYGMaPHw9QurlZhw4dsFgsXHzxxeUe+/rrr1O3bl1iY2MZPnw4RUVF5/wZVIXDiYXVamXcuHEMHTqUgIBKDXiIiPiWikyD2LW4HIIijH1Fdi11bhw2GxTmmXOrYJJ06NAhZs6cyfDhwwkPDz/tY/66YODZZ59l0KBBpKWlMWzYMCZOnMiIESN4+OGHWbt2Lf/3f//Hrbfeyrx58xz+J3vqqad44403WLZsGQEBAQwbNqz0a9999x3PPvssL730EsuWLaNu3bqMHj3a4WuUNWfOHNavX8+sWbOYMmVKhZ6zZMkSAGbPns3evXvLzSbMmzePLVu2MG/ePD799FPGjx9fmpC4isOZwezZs9m5c2e5f1wRETmLc60IKSsoDFr1hzXfGqMWiec5L46i4/Dy2eviXObJPRB0+kShrM2bN2Oz2WjZsmW5+2vVqlU6GjB8+PBy24zfcMMN5V6TbrjhBm655RbuueceAB566CEWL17M66+/Ts+ePR0K+6WXXiptqfD444/Tv39/8vPzCQkJ4a233mLYsGHcfvvtALz44ovMnj37rKMW5xIeHs5HH33k0BSIffokNjaWOnXqlPtajRo1GDVqFP7+/rRq1Yr+/fszZ86cctMvzubwiEWfPn2w2WylQzQiInIWNtuZW3mfiX11SPpEKCl2RVQe76+jEkuWLGHVqlUkJydTUFBQ7mupqanlPl+/fj1du3Ytd1/Xrl1Zv369w3G0a9eu9Lhu3boAZGVllV6nS5cu5R7/188d1bZtW6fWVSQnJ+Pv71/6ed26dUvjdxXNZYiIuNKR7Ubhpn/Q2Qs3y2pyMYTFQt4B2DYfmvVyTiyBYcbIgRkCwyr0sGbNmmGxWNiwYUO5+5s0aQJAaGjo355zuimTvyYmNput9D4/P7/S++zOVHdQthDU/nyr1XrO76OyzvS92P4ylVTROom/FrJaLBaXxg/ahExExLXs0yBxZ+i4eTr+gZB8pXHszJ4WFosxHWHG7SyNFMuKjY3l0ksvZdSoUeTl5VXq20xKSmLhwoXl7lu0aBFJSUnAqamDvXv3ln69Mks1k5KSWLx4cbn7/vq5M9SuXbtcrJs2bSq35NY+wlFSUuL0a1eGRixERFzJ0WkQu7ZXw9KPYP3PMOBNCPz7O/XqavTo0XTt2pXU1FSee+452rVrh5+fH0uXLmXDhg106tTprM//17/+xTXXXEPHjh3p1asXP//8Mz/++COzZ88GjFGPCy64gFdeeYVGjRpx8OBBnn76aYfjHDFiBEOHDiU1NZVu3brx5Zdfkp6eXjq64iyXXHIJo0aN4oILLsBqtfLYY4+VG4mIi4sjNDSU6dOnU79+fUJCQoiOjnZqDI7QiIWIiCvZRyzO1nHzdOqfB9GJUJgLGTOcHpYna9q0KStXrqR379488cQTtG/fntTUVN59910eeeQRXnjhhbM+f/Dgwbz99tu89tprJCcn88EHHzBu3LhyyzA/+eQTioqKSE1NZcSIEaXLVB1x7bXX8swzz/DYY4/RqVMnduzYwd133+3wec7ljTfeIDExke7du3PDDTfwyCOPEBZ2amopICCAd955hw8++ICEhAQGDRrk9BgcYbH9deLGxXJycoiOjiY7O5uoqCh3XlpExL1sNni1kVFjced8x0ctZj0Lv79ldOS89guHL5+fn8+2bdto3LgxISEhDj9ffM/Zfmcq+vqtEQsREVepTOFmWfbVIRkz4cRRJwYm4jpKLEREXKVc4WYllhDGJ0PtJCgpgA0Va5YkYjYlFiIirlLacdPB+go7iwXaDjGO0753TkwiLqbEQkTEVSq7IqSsNif3Dtm2AHL3VzUiEZdTYiEi4go2G+xdbRxXpJX3mdRsDPU7g81qdOIU8XBKLEREXKGqhZtl2Ys4Kzkd4ubFf+LFnPG7osRCRMQV7PUV8cmVK9wsK/lKsPjB7mVweGuFn2ZvolS2S6PI2dh/V/7aCtwR6rwpIuIKjuxoei4RcdC4B2ydB2kToMe/KvQ0f39/YmJiSjedCgsL+9seGiJgjFQcP36crKwsYmJiym1c5iglFiIirlBauFnJFSF/1fbqk4nF99D9kQrvvWHfRtvVO1pK9RATE/O3rdcdpcRCRMTZbLYyrbxTnHPOpAEw5UE4uBH2r4U6bSv0NIvFQt26dYmLi6vwjpjimwIDA6s0UmGnxEJExNmObIP8bKNws3aSc84ZEg0tLoP1k41RiwomFnb+/v5OedEQORcVb4qIOJt9GsQZhZtltT3Z0yJtAlitzjuviBMpsRARcbaqdtw8k+Z9IDgKcnZB5mLnnlvESZRYiIg4mzNXhJQVGGrsdAqQ9oNzzy3iJEosREScqWzHTWcVbpZlnw5JnwglKsYUz6PEQkTEmUoLN4OdV7hZVqPuEB4HJw7DlnnOP79IFSmxEBFxJmd23Dwd/wCjEydox1PxSEosREScyRk7mp6Lfe+QDb9Aodp1i2dRYiEi4kyljbGcvCKkrPqpENMQivIgY5rrriNSCUosREScxWaDPU7YKv1cLJYyO55qdYh4FiUWIiLOcngrFJws3IxzQeFmWfbVIZtmwfHDrr2WiAOUWIiIOIt9GiQ+Gfwrv+10hcQlQXwbsBYZbb5FPIQSCxERZ3H2jqbnUtriW9Mh4jmUWIiIOEtpK+8U91yvzRDj4/aFkLPHPdcUOQclFiIizmCzwd41xrErCzfLimkADboANlj7o3uuKXIOSixERJzBnYWbZdlHLdQsSzyEEgsREWewT4PUaeP6ws2ykq8Ei79ROHpws/uuK3IGSixERJzBVTuankt4LWh6iXG8VkWcYj4lFiIizuCOVt5nUtos63uj1kPEREosRESqymots1W6m5aaltWqHwSEwKHNp0ZOREyixEJEpKqObIOCnJNbpbdy//WDI6FlX+NYPS3EZEosRESqyqzCzbLs0yFrJ4C1xJwYRFBiISJSde7Y0fRcmvWGkGjI3Qs7FpkXh/g8JRYiIlVlL9x094qQsgKCofUg41g9LcRESixERKqiXOFmiqmh0Obk3iHrfoLiAnNjEZ+lxEJEpCrshZsBIeYUbpbVqBtE1IH8o7B5jrmxiM9yOLHYvXs3N910E7GxsYSFhZGSksLy5ctdEZuIiOezF27Gm1i4aefnf6rFt5pliUkcSiyOHDlC165dCQwMZNq0aaxbt4433niDmJgYF4UnIuLh3L2j6bnYt1LfMBUKjpkbi/ikAEce/Oqrr5KYmMi4ceNK72vUqJGzYxIR8R72+gozCzfLSugANZsYm6JtnArtrjE7IvExDo1YTJ48mdTUVK6++mri4uLo0KEDH3744VmfU1BQQE5OTrmbiEi1YHbHzdOxWMq3+BZxM4cSi61btzJmzBiaN2/OjBkzuOuuu7j//vv57LPPzvickSNHEh0dXXpLTEysctAiIh7h8FbPKdwsy746ZMtcyDtkbizicyw2W8V3rAkKCiI1NZVFi041X7n//vtZunQpf/zxx2mfU1BQQEHBqWVPOTk5JCYmkp2dTVRUVBVCFxExWdoPMOE2qJcKd3jYKowPuhujKf3fhM63mR2NVAM5OTlER0ef8/XboRGLunXr0rp163L3JSUlsXPnzjM+Jzg4mKioqHI3EZFqobRw00OmQcoqnQ7R6hBxL4cSi65du7Jx48Zy92VkZNCwYUOnBiUi4hXM3Cr9XJKvAiywcxEczTQ7GvEhDiUWDz74IIsXL+bll19m8+bNfPXVV4wdO5bhw4e7Kj4REc9UtnDTU1aElBVdDxp2NY7XTjA3FvEpDiUWnTt3ZuLEiXz99de0adOGF154gbfeeosbb7zRVfGJiHimw1uhMNfzCjfLsve0ULMscSOH+lgADBgwgAEDBrgiFhER71G6VXpb8Hf4T6l7tB4EU/8F+9IgawPEeWgCJNWK9goREakM+1bpnjgNYhdWE5r1Mo41aiFuosRCRKQyPLlws6yyzbIq3l1ApNKUWIiIOMoTO26eScu+EBgGR7bD7hVmRyM+QImFiIijDm85WbgZCrVamh3N2QWFQ6v+xrFafIsbKLEQEXGUfRqkThvPLdwsyz4dsnYCWEvMjUWqPSUWIiKOshduevo0iF2TnhBaA/KyYNsCs6ORak6JhYiIo+xLTT15RUhZAUHQerBxrBbf4mJKLEREHGG1wt41xrGnrwgpyz4dsv5nKMo3Nxap1pRYiIg4wpsKN8tq0AWi6kFBNmyeZXY0Uo0psRARcURp4aYHd9w8HT8/aHOVcazVIeJCSixERBxRulV6iqlhVIp9OmTjdMjPMTcWqbaUWIiIOMIbWnmfSZ12UKsFlBTAhl/MjkaqKSUWIiIV5U0dN0/HYinf4lvEBZRYiIhU1KHNUHjsZOFmC7OjqZw2Q4yPW3+FY1mmhiLVkxILEZGKsk+DeFvhZlmxTSGhI9hKIH2S2dFINaTEQkSkokp3NPXCaZCyNB0iLqTEQkSkorx5RUhZba4CLLBribHrqYgTKbEQEakIqxX2ney46Y0rQsqKrAONuxvHayeYG4tUO0osREQqojoUbpbV9h/GR+0dIk6mxEJEpCJK+1e0897CzbKSBoJ/EGStg/3pZkcj1YgSCxGRivC2HU3PJbQGNO9jHGvUQpxIiYWISEWUrghJMTMK5yo7HWKzmRuLVBtKLEREzsVacqpw09uXmpbV4nIIioDsnZC5xOxopJpQYiEici72ws3AsOpRuGkXGAqtBhjH6mkhTqLEQkTkXMpule7nb2ooTmdvlpU+EUqKzY1FqgUlFiIi52JfEVKdpkHsmvSAsFpw/CBs+9XsaKQaUGIhInIu9hGL6rIipCz/QEi+0jjW6hBxAiUWIiJnYy0ps1V6iqmhuIx9dcj6n6HohLmxiNdTYiEicjaHNkNRXvUr3Cyr/nkQ3cAoUM2YbnY04uWUWIiInE1p4Wa76le4aefnB22HGMeaDpEqUmIhInI21WVH03Oxrw7ZNBNOHDU1FPFuSixERM6mdI+QFDOjcL34ZIhrDSWFRq2FSCUpsRARORNrCeythh03z6SNfTpEzbKk8pRYiIicycFNJws3w6FWc7OjcT376pBtCyB3n7mxiNdSYiEicib2aZDq2HHzdGo0MlaIYDM6cYpUghILEZEzqY47mp6LvYhT0yFSSUosRETOpDq38j6T5MFg8YPdy+HQFrOjES+kxEJE5HTKdtys7itCyoqIgyYXG8drJ5gaingnJRYiIqdzcBMUHfedws2yyk6H2GzmxiJeR4mFiMjplPavqMYdN8+k1QDwD4aDGbAvzexoxMs4lFg899xzWCyWcrc6deq4KjYREfPYO2760jSIXUgUtLzcOFYRpzjI4RGL5ORk9u7dW3pLS1M2KyLVkC+uCCmrzcmeFmsngNVqbiziVQIcfkJAgEYpRKR6s5bAPh/quHk6zftAcBTk7Iadf0CjrmZHJF7C4RGLTZs2kZCQQOPGjbnuuuvYunWrK+ISETFP2cLN2GZmR2OOwBBIusI4XqsdT6XiHEoszj//fD777DNmzJjBhx9+yL59+7jwwgs5dOjQGZ9TUFBATk5OuZuIiEcrra/wwcLNsuwtvtMnQnGhubGI13Aosejbty9Dhgyhbdu29O7dm19++QWATz/99IzPGTlyJNHR0aW3xMTEqkUsIuJqvrKj6bk07g7hcXDiCGydZ3Y04iWqtNw0PDyctm3bsmnTpjM+5oknniA7O7v0lpmZWZVLioi4Xmnhpo/WV9j5+UObq4xjrQ6RCqpSYlFQUMD69eupW7fuGR8THBxMVFRUuZuIiMcqV7iZYmooHsHeLGvDL1CYZ24s4hUcSiweeeQR5s+fz7Zt2/jzzz/5xz/+QU5ODkOHDnVVfCIi7nUwQ4WbZdXrZOx6WnQcNk4zOxrxAg4lFrt27eL666+nZcuWXHXVVQQFBbF48WIaNmzoqvhERNzLPg1St71vF27aWSxlWnxrdYicm0N9LL755htXxSEi4hnsK0I0DXJKm3/Agtdg8yw4fhjCapodkXgw7RUiIlKWVoT8XVwriG8L1mJY95PZ0YiHU2IhImJnLTm16Zavrwj5q7ZlWnyLnIUSCxERO3vhZlCECjf/qs0Q4+P2hZC929xYxKMpsRARsbPXV9RpB37681hOTCI06ALYIP1Hs6MRD6b/OSIidr6+o+m52KdD1CxLzkKJhYiInb1wU/UVp9f6SvALgL2rjY3aRE5DiYWICEBJMew92XFTK0JOLzwWml5iHKunhZyBEgsRETAKN4tPqHDzXEqbZX0PNpu5sYhHUmIhIgJl+le0V+Hm2bTsBwGhcHjLqWJXkTL0v0dEBMq08k4xMwrPFxwBLfsax5oOkdNQYiEiAmrl7Qj7dEj6j0ZTMZEylFiIiJQUq+OmI5r1hpAYyN0LO343OxrxMEosRERKCzcjoWZTs6PxfAFB0PoK41g9LeQvlFiIiNinQeqq42aF2adD1v0ExQXmxiIeRf+DRES0o6njGnaFyLqQnw2bZ5sdjXgQJRYiIqWtvFVfUWF+/qc2JtPqEClDiYWI+LZyhZsppobideyJxcZpUJhnbiziMZRYiIhvO7hRhZuVldABYhoa/36bZpkdjXgIJRYi4ttKG2Op46bDLBZIHmwcr5tkZiTiQfS/SER8W+mOpilmRuG9Wg8yPmbMgMLj5sYiHkGJhYj4ttKlpimmhuG1EjpCTAMoOq7VIQIosRARX1ZSDPvWGscasagci+XUqIWmQwQlFiLiy1S46RytBxsfN06HohOmhiLmU2IhIr6rdBpEhZtVUq8TRCdCUR5snmN2NGIy/U8SEd9V2hgrxcwovJ+mQ6QMJRYi4rtKV4So42aV2ROLjdOhKN/cWMRUSixExDeV7bipFSFVVy8VoupBYS5smWt2NGIiJRYi4psObIDi/JOFm03Mjsb7+flpOkQAJRYi4qvKNsZS4aZzlE6HTNNW6j5M/5tExDeVbeUtzlH/PIhMgIIc2DLP7GjEJEosRMQ32ZeaqnDTefz8oPUVxrGmQ3yWEgsR8T0lxbD/ZMdNFW46l306ZMNUTYf4KCUWIuJ77IWbwVEq3HS2xAsgog4UZMPW+WZHIyZQYiEivkcdN11H0yE+T/+jRMT32FeEqHDTNUqnQ6ZAcaG5sYjbKbEQEd9T2spbhZsu0aALhMdBfjZsW2B2NOJmSixExLeUFJ0q3FRi4Rp+/mWmQyaaG4u4nRILEfEtZQs3azQ2O5rqq3Q65BcjmROfocRCRHxL2cZYKtx0nYZdIbw2nDii6RAfo/9VIuJbyrbyFtfx84ekgcaxVof4FCUWIuJbSpeappgahk+wT4esn6LpEB9SpcRi5MiRWCwWHnjgASeFIyLiQiVFsE+Fm27TsBuExcKJw7B9odnRiJtUOrFYunQpY8eOpV27ds6MR0TEdQ5sgJICCI5W4aY7+AdoOsQHVSqxOHbsGDfeeCMffvghNWrUcHZMIiKuUVq42U6Fm+5SOh3ys7FHi1R7lfqfNXz4cPr370/v3r3P+diCggJycnLK3URETFG6o2mKqWH4lEYXQWhNOH4IdvxudjTiBgGOPuGbb75hxYoVLF26tEKPHzlyJM8//7zDgYmIcxQWW/l88Q6+X5YJQERwAOHBAUSEBBARZHwMDw4gItifiOBAwoP9iQwJIPzk1yKCA0qfExzgh8ViMfk7qoLSFSGqr3Ab/0Bo1R9Wfm5MhzTpYXZE4mIOJRaZmZmMGDGCmTNnEhISUqHnPPHEEzz00EOln+fk5JCYmOhYlCLiMJvNxtS0fbw6fQM7Dx93yjkD/S2EBxtJR2RpQlI++TCSkVNJStmvlX2O25OUsoWbWhHiXsmDjcRi/c/Q73VjKapUWw4lFsuXLycrK4tOnTqV3ldSUsKCBQsYNWoUBQUF+PuX/4UJDg4mODjYOdGKSIUs236Yl6auZ+XOowDUjgzm/l7NaRQbxrH8Yo4VGLe8gmJyT3407i8xjv/yteOFJQAUldg4eryIo8ervnQwwM9ijJaUSVLCgwOIDA44mZAEGgnKXxKYyJBA2taLJjTIwRenrPWnCje1Vbp7Ne4BITGQdwB2LILGF5kdkbiQQ4lFr169SEtLK3ffrbfeSqtWrXjsscf+llSIiHttO5jHq9M2MD19HwChgf78X48m3HFRE8KDHZ75LFVitZFXaCQff09GikuTkWMFJRwrKCKvoITc/OKzJinF1sonKRHBAQxsX5erUxPpkBhTsZGP0h1N24E3T+d4I/9AaDUAVn1hTIcosajWHPpLExkZSZs2bcrdFx4eTmxs7N/uFxH3OZxXyDtzNvHF4h0UW234WeDazok82LsFcVEVm7Y8G38/C1EhgUSFBFb5XPYkpWxiUpp85JdPUk4dn0xqCovZl51PVm4BXy/J5OslmTStHc41qYlc2bEecZFn+V61o6m5kgefTCwmQ9//ajqkGqv8WxgRMV1+UQnjft/O6HmbyS0wlvL1bFmbJ/ol0SI+0uToTq9ckhLt+POtVhuLtx3ih2W7mLp2L1sO5DFy2gb+O2MjPVvW5urURC5pFUeg/18WvWlFiLka94CQaMjLgp2LoVFXsyMSF7HYbDabOy+Yk5NDdHQ02dnZREVFufPSItWG1Wrjp9W7eW36RvZk5wPQum4UT/VPomuzWiZH5z45+UX8smYv3y3LLK0nAYgND+LKDvW4OjWRlnUijcLNl+sZNRb3rYDYpuYF7csm3g2rv4Lz7oR+r5kdjTiooq/fSixEvMyizQd5edp61u42esLUjQ7hX5e1ZHBKPfz8fLd2YHNWLt8v28WEFbs5eKyg9P729aO5s0Ue/RddYxRuPr5DNRZmyZgBX10DEXXgofVqUuZlKvr6rakQES+xaX8uI6dtYO6GLMAoYLynZ1OGdW1MSKDmq5vFRfJEvyQeuawl8zce4LtlmczdkMXqXdnM3zuP/oGQ4d+UA1sO0aVJrE8nYaZpcrGR3B3bB5l/QsMuZkckLqDEQsTDZeXm879Zm/h26U6sNmOZ5o3nN+D+Xs2JjdBS7r8K9Pejd+t4ereO5+CxAiat3E38b59BIczNSeCVj/6kXkwo/+hUn390qk9izTCzQ/YdAcHQsi+s+cZYHaLEolpSYiHioY4XFvPhgm18sGBL6RLNy5LjeezyVjSpHWFydN6hVkQwt1/UBNv6LNgD0U07E7kjgN1HT/D2nE28PWcTFzaN5ZrURC5vU0cjP+6QPPhkYvETXDZS0yHVkBILEQ9TYrXxw/JM3piZQVauUSvQPjGGp/sn0blRTZOj80LFhVj2pwNw/aAruDKyITPS9/Hdskx+33yIRVuMW+RPAQxsn8A1qYm0rx/t3a3LPVnTSyAoEnL3wq6l0OB8syMSJ1NiIeIhbDYb8zMOMHLqBjbuzwUgsWYoj17WigHt6uqFrrIOnOy4GWJslR5isTAopR6DUuqRefg4E1bs4vtlu9h99ARf/bmTr/7cSfO4CK5JTWRwh3rUjtR0k1PZp0PSvjOmQ5RYVDtaFSLiAdL3ZDNy6gYWbj4IQHRoIPdd0oybuzQkOEDD81Wy/FP4+X5o3B2G/nzah1itNhZvPcR3yzKZtnYfBcVWwKhn6dkqjmtSE7m4Ze2/98aQytnwC3xzA0TVgwfWajrES2hViIgX2Jt9gtdnZPDjyl3YbBDk78fQCxtyb8/mRIdVvculUKEdTf38LFzYrBYXNqvF8yeKmLJmD98t28XqzKPMWrefWev2UysimKs61uPqTvVp7qHNx7xG014QFAE5u2H3ckjsbHZE4kRKLERMkJtfxPvzt/DRb9tK3x0PbJ/Ao5e11CoFZ7O38q7gjqbRoYHceH5Dbjy/IRn7c/l+WSYTVxq9McYu2MrYBVtJSYzhmtREBrSv65Q25z4nMARaXA5rfzCmQ5RYVCuaChFxo6ISK98s2clbszdxKK8QgPMa1eTJ/kmkJMaYG1x1VFwII+tBSSHcv7LSu5oWlViZtyGL75btYt7GLEqsxp/NkEA/+rapy9Wp9bmgsXpjOGT9z/DtTRCdCA+kqWmZF9BUiIgHsdlszFq3n1embWDrwTwAmtQK5/G+rbi0dbwKM13lwHojqThZuFlZgf5+9EmuQ5/kOmTl5jNp5W6+W7aLzVnHmLhyNxNX7iaxZij/6JjIkE71qF9Do07n1Kw3BIZDdibsXgH1O5kdkTiJEgsRF1uVeZSXf1nPku2HAagZHsSDvZtz3XkNVAzoamWnQZyUvMVFhnBn96bccVETVmUe5btlu5iyeg+Zh0/wv9kZvDUng65Na3F1an0uS1ZvjDMKDIUWl0H6j7BuohKLakSJhYiLZB4+zn9nbOTn1XsACA7w4/aLGnNXj6ZEal7ePVy4o6nFYqFDgxp0aFCDZwa0Znr6Xr5buos/th5i4eaDLNx8kMiQAAalGL0x2tZTb4y/SR5sJBbpP8GlL2g6pJpQYiHiZNnHixg1bxOfLtpBYYkViwWu6lCfh/u0ICEm1OzwfEsFVoQ4Q2iQP1d2qM+VHeqTefg43y/fxYTlRm+MLxbv5IvFO2kZH8nVqfW5skM9tWK3a3YpBIZB9k4jCazX0eyIxAlUvCniJAXFJXz+xw7enbuZ7BNFAHRtFsuT/ZJITog2OTofVK5wcxXUrHyNRWVYrTYWbTF6Y0xP30dhmd4YvZKM3hg9WtQmwNenw74baqwM6ToCLv2P2dHIWah4U8RNbDYbv6Tt5b/TN7Lz8HEAWsZH8ni/VlzcoraGv82Ste5k4WYM1Gjk9sv7+Vno1rwW3ZrXIvt4EZPX7OH7ZZms2ZXNjPT9zEjfT6s6kbx+dXva1PPhxDN5sJFYpE+C3s9rOqQaUGIhUgVLtx/mpV/WsyrzKABxkcE83KcF/+iUiL+WHprLPg1St73pL1bRYYHcfEFDbr6gIRv25fD9sl38sHwXG/blMvi937mnZzPu7dmMoAAfHL1o3gcCQuHoDti72iX1MOJePvhbLFJ1Ww8c4/8+X8bV7//BqsyjhAX582DvFvz6r4u5tnMDJRWewL4ixMX1FY5qVSeKfw9ozZyHe9C3TR2KrTbembOJK0YtZO3ubLPDc7+gcGh+qXG8bpKpoYhzKLEQccChYwU8+9Na+vxvATPS9+NngevPa8Cvj1zMiN7NCQvSIKDHKC3cTDEzijOqFRHMmJs6MeqGDtQMDyodvXhzVkZpPYbPSB5sfEyfBO4t+xMX0F9BkQrILyrhk9+3MWbeFnILigG4pFUcj/dtRQvtG+F5igvh5FbpFW3lbZYB7RK4oEksz/y0lqlp+3hnziZmpu/zrdqL5pdBQAgc2Qb70qBuO7MjkirQiIXIWVitNn5csYtLXv+V/07fSG5BMckJUXx1+/l8cktnJRWeyuTCTUfVighm9I2deO+Gjr45ehEcYXTiBE2HVANKLETOwGaz8eiENTz03Wr2ZOeTEB3Cm9e05+d7u3Fhs1pmhydnU3YaxItWGfRvV5eZD3anX1sfrL1IvtL4qOkQr6fEQuQM3pq9iR+W78Lfz8K/LmvJ3Ecu5qqO9bXRlDewd9z08GmQ0/HZ0YsWl4F/MBzecmoaS7ySEguR0/h+WSZvz9kEwAuD2jC8ZzPt+eBNSleEpJgZRZX43OhFcKSmQ6oJJRYif7Fw00Ge+DENgHsubsoN5zcwOSJxSHGhUWMBHrfU1FGnG70Y9N7vvDlzY/UcvdDqkGpBiYVIGRv25XD3F8spttq4on0Cj/RpaXZI4qiyhZsxDc2Oxinsoxf929alxGrjnbmbq+foRYvLwT8IDm2CrPVmRyOVpMRC5KR92fncOm4puQXFnNe4Jq9d3U71FN6o7I6mXlS4eS61IoJ578aO1Xv0IiQKmvYyjjUd4rWUWIgAxwqKuXX8UvZm59Okdjhjb+5EcIBqKrySm3Y0NUv/dnWZVZ1HL+zTIet+MjUMqTwlFuLzikqsDP9yBev35lArIohPbz2PmLAgs8OSyrIXbnrhipCKiq3Ooxct+4JfIBzYAFkbzI5GKkGJhfg0m83GMz+tZX7GAUID/fl4aGcSa4aZHZZUVnHBqaWKXrwipKKq5ehFSDQ0vcQ41nSIV1JiIT5t9K9b+HpJJn4WeOf6DrRPjDE7JKmKrHVgLYLQGtWmcPNcyo5exJYZvXjDm0cvNB3i1ZRYiM/6adVuXpuxEYBnByZzaet4kyOSKis7DVKNCjcronTlSDtj9OJdbx69sE+HZK2DAxlmRyMOUmIhPmnx1kP86/s1ANzerTFDL2xkbkDiHGVXhPig2Ihg3ruhI6Nv9PLRi9Aa0ORi41jTIV5HiYX4nM1Zudz52TIKS6z0bVOHJ/slmR2S6+Vnw29vVv/eAPYVIdW4cLMi+rWtBqMXmg7xWkosxKdk5eZzy7il5OQX07FBDP+7NsU3elXMegbmPA9jL4Zl46pnV8PiAthfPTpuOsPZRi8KikvMDu/cWvYDvwDYvxYObjY7GnGAEgvxGccLi7lt/DJ2HTlBo9gwPhra2Tf2/8jPgTXfG8fF+TDlAfjhVmMUozopV7ipNux2px29ePd30nZ5+M8/rCY07mEcr5tobiziECUW4hNKrDbu/3olabuzqREWyPhbz6NmuI/0qljzLRTlQa2W0Pt5411g+kT4oDvsXmF2dM5TdkdTHyvcPJe/jl5s3J/L4NFeMHqh6RCvpMRCqj2bzcbzP6cze30WwQF+fDS0M41qhZsdlnvYbMbUB0DqMOj2ANw6HaIbwJHt8HEf+OO96jE1UrqjqaZBzsTrRi9aDQCLP+xLg0NbzI5GKkiJhVR7H/22jc/+2IHFAm9dm0KnhjXMDsl9MpdAVjoEhEL764z7EjvDXQsgaaAxdTDjSfj6ejh+2NxYq6q0lXeKmVF4PK8avQirCY27G8daHeI1lFhItfbLmr28NNVYCfFUvyT6tq1rckRutuxj42PbIRAac+r+0BpwzefQ73VjN8mMafB+N9jxhylhVlnZwk0fXxFSUV4zeqHpEK+jxEKqrWXbD/Pgd6sAGNqlIbd1a2xuQO6WdwjSJxnHqcP+/nWLBc67A26fA7HNIGc3jO8PC14Dq4e9cz2X/eknCzdrqnDTAWcavXh9hgeNXrQaaEyH7F0Nh7eZHY1UgEOJxZgxY2jXrh1RUVFERUXRpUsXpk2b5qrYRCpt28E87vhsGYXFVnonxfPMwGQsvlbQt/orKCmAuu0hoeOZH1e3Hdz5K7S7FmwlMPdF+OIqyN3vtlCrrOw0iK/9nJ3APnox4OToxah5HjR6ER4LjboZx5oO8QoOJRb169fnlVdeYdmyZSxbtoxLLrmEQYMGkZ6e7qr4RBx26FgBt4xbwpHjRbSvH82713fA3xd6VZRltZYp2rzt3C+2wZFw5QcwaDQEhsHWX+H9rrBlrstDdQof2NHU1WIjghl1Q0fGeOLohaZDvIpDicXAgQPp168fLVq0oEWLFrz00ktERESwePFiV8Un4pD8ohJu/2wZOw4dJ7FmKB8N7UxokA/0qvirbfPh8BYIjoI2Qyr2HIsFOtxojF7EtYa8A/D5VTD7eSgpdmm4VebjrbydqW/busx6qIdnjV60GggWP+PnfGS7eXFIhVS6xqKkpIRvvvmGvLw8unTpcsbHFRQUkJOTU+4m4golVhsPfLOKlTuPEh0ayLhbzqN2ZLDZYZlj2SfGx3bXQnCEY8+t3RLumAudbgVssPBNGN8PjmY6PUynKC441apcS02domZ4kGeNXkTUhoZdjWONWng8hxOLtLQ0IiIiCA4O5q677mLixIm0bt36jI8fOXIk0dHRpbfExMQqBSxyJi9PXc/09H0E+fsx9uZONItz8AW1usjZCxt+MY5PV7RZEYGhMPAt+Mc4Y9Qj809j1ciGqU4L02nKFm5G6++LM51p9GLNrqPuD0bTIV7D4cSiZcuWrFq1isWLF3P33XczdOhQ1q1bd8bHP/HEE2RnZ5feMjM99F2PeLVxv2/j44VGxfjr17Tn/CaxJkdkopVfGEWYDbpA/JmT/gppcxX83wJjJCD/KHxzPUx73Bgl8BRlp0FUuOl0pxu9uHL0Il6bscG9oxdJVwAW2L0cju5033XFYQ4nFkFBQTRr1ozU1FRGjhxJ+/btefvtt8/4+ODg4NJVJPabiDPNSN/Hf6YYye2jl7fkivYJJkdkImsJLB9vHFd2tOKvajaGYTPhguHG53+OgY8v9ZxOiKUrQjQN4kr20YuB7RMosdp4b94W945eRMRpOsRLVLmPhc1mo6DAg969iE9ZlXmUEd+sxGaD689rwN09mpodkrk2zYScXca0QNIVzjtvQBBc/jJc/43RXGvvavigB6T94LxrVJZWhLhNzfAg3r2+A+/f1JFaEadGLz5csNU9AWg6xCs4lFg8+eST/Pbbb2zfvp20tDSeeuopfv31V2688UZXxSdyRjsPHee28UvJL7LSs2VtXhjkg70q/spetNnhRggMcf75W/aFuxYa0yyFuTDhNph8HxQed/61KqIo39jVFLQixI0ub1OXmQ+eGr14edp6Vu484voLJw0ELLBrKWTvcv31pFIcSiz279/PzTffTMuWLenVqxd//vkn06dP59JLL3VVfCKndSSvkFvGL+FQXiHJCVGMuqEjAf4+3kj2yA7YNMs47nSr664TXR+GToHu/wIssOIz+PASyNrgumueSVY6WItVuGkC++jFVR3qYbPBEz+mUVRide1FI+sYSS1o1MKDOfSX+OOPP2b79u0UFBSQlZXF7NmzlVSI2+UXlXDn58vYeiCPhOgQPrmlM+HBAWaHZb7l4wEbNOkJsS6eEvIPgEuehpsnQngcHFgPYy+GFZ+7d6fUsjua+vpolUme6p9ETFggG/blMu53N7Tc1nSIx/Pxt3jibaxWG498v5ql248QGRzAuFvPIz7KBUP+3qa4EFZ+bhw7q2izIpr2hLt/N5KZ4hMw+V748Q4oyHXP9bWjqeliI4J5sm8SAP+btYldR1w8LWavHcr8E7J3u/ZaUilKLMSr/HfGRqas2UuAn4X3b+5EyzqRZofkGTZMMTplRtQx6iDcKSIObvoRej1jbBaV9j180P3UaIIr2ZeaqnDTVFen1ue8xjU5UVTCMz+lY3PlqFVUXUi8wDheP9l115FKU2IhXuOLxTt4f76xxPHVIe3o2qyWyRF5EHvRZqeh4B/o/uv7+cFFD8OtUyGqPhzeaixJ/fMD102NFOWr46aHsFgsvHxlGwL9LczdkMX0tftce0FNh3g0JRbiFeZu2M8zP60F4MHeLRjSqb7JEXmQAxmw/TdjL4WO/zQ3lgYXwF2/Qct+UFII0x6Fb2+CEy5YMWAv3AyLNQpKxVTN4iJLl3s/OzmdnPwi113MPh2yc7HRaVY8ihIL8Xhpu7K596uVWG3wj071ub9XM7ND8izLT+5i2uJyz3iBDasJ130Fl78K/kHGNM37F0HmEudep+w0iAo3PcI9PZvRKDaMrNwC3pix0XUXiq4H9c8DbJoO8UBKLMSj7TpynGGfLuV4YQndmtVi5FVt1auirKITsOpL49idRZvnYrHABXfBbTOhRmPIzoRPLoeF/zO2dHeGsitCxCOEBPrz4uC2AHy2eAerMo+67mKaDvFYSizEY2WfKOLWcUs5kFtAqzqRjL6pI4G+3qvir9InQn42xDSApr3MjubvEjoYe420GWLsXzL7OfjyH3DsQNXPrRUhHqlb81pcWaa3RbGrelvYp0N2LIJcF9d0iEP0V1o8UmGxlbs+X86mrGPERwUz7tbORIWYUJTo6ZZ+bHzsdKtRQOmJQqJgyMcw8B0ICIUtc+D9rrB1fuXPWbZwUytCPM5T/ZOIDg1k/d4cxv2+3TUXiUmEeqkY0yE/u+YaUike+pdIfJnNZuPxCWv4Y+shwoP8+eSWztSNDjU7LM+zdzXsXgZ+gdDhJrOjOTuLxVixcsdcqN0Kju2HzwbB3JegpNjx8+1X4aYnqxURzJP9WgHw5qwM1/W20HSIR1JiIR7nf7My+HHlbvz9LIy+qRPJCdFmh+SZlp0s2kwaaPSS8AbxreGOedDhZsAGC/4Lnw50vNHRXvtW6eq46amu7pTIeY2M3hbPuqq3RetBxscdv8OxLOefXypFiYV4lO+WZvLO3M0AvDS4DT1a1DY5Ig9VkGs0ogLofJu5sTgqKAwGjYKrPoKgCNi5CN7vBhkzKn4O7Wjq8fz8LLx0srfFnA1ZzEh3QR1ETANI6Ag2q1aHeBAlFuIxFmQc4MmJaQDc27MZ153XwOSIPNiab6HwGNRqAQ27mh1N5bS72ijsrNMOThyGr66BGU8Z7cnPpXRFSIorI5Qqah4fyV1lelvkuqK3haZDPI4SC/EI6/bkcM+XKyi22hicksDDfVqYHZLnstlg6clOm6nDvHsqILYp3D4bzvs/4/M/RsEnl8Hhs2xmVZRvbHoGWmrqBYb3bEbD2DD25xTwxswM51/APh2yfaFzVhtJlSmxENPtzT7BsPFLOVZQzAVNavLqP9qpV8XZ7FpqdJ0MCIX215kdTdUFBEO//8K1X0JINOxZYew1kj7x9I8vLdysBVH13BurOMzobdEGgE//2M5qZ/e2qNHImBKzWWGDVod4AiUWYqrcfKNXxb6cfJrFRfDBTakEB/ibHZZns+8L0mYIhNYwNxZnShoAdy00OioW5MD3t8CUB40mYGXtWWF8TEjx7tEaH3JR89oMTklwXW8LTYd4FCUWYpqiEiv3fLmCDftyqRURzLhbOhMdpl4VZ3X8MKz90Tj2pE6bzhLTwNjIrNuDxufLPoGPehv7odiVNsbSNIg3eXpAa6JDA1m3N4fxi7Y79+T26ZBtv0HeIeeeWxymxEJMYbPZeGpiGr9tOkhooD+f3JJKYs0ws8PyfKu+gpICo+CxXkezo3EN/0Do/RzcNMGY7ti/Fsb2ML53gD2rjY9aEeJVakUE80TfU70tdh89cY5nOKBmE+P/hK1E0yEeQImFmGLU3M18t2wXfhYYdUMH2tWPMTskz2eznZoG8faizYpo1hvu/h0ad4ei4zDpbphwR5nCzRRTwxPHXZOaSOdGNTheWMKzP611bm8LTYd4DCUW4nYTV+7ijVnG0PbzVyTTKyne5Ii8xLYFcHgLBEVC26vNjsY9IuvAzZOg51PGtvBp36lw04v5+Vl4+cq2BPpbmL0+ixnp+5138taDjY9b5xtThmIaJRbiVou2HOTRH9YA8H/dm3Bzl0bmBuRNlp3cF6T9tRAcYW4s7uTnDz0ehaFTILKucV/91Oo/YlNNNY+P5M7uTQB4zpm9LWKbQnzbk9MhU5xzTqkUJRbiNhn7c/m/z5dTVGKjf7u6PHZ5K7ND8h65+2DDL8Zxp1vNjcUsjbrCXb9Dr2ehz0tmRyNVcN8lzWlQM4x9OfnO7W2RfLKIU9MhplJiIW6RlZPPreOWkptfTGrDGrxxdXv8/PSOs8JWfm5MASSeD3XamB2NecJj4aKHoFYzsyORKijb2+KzP7azZtdR55y4dDrkV02HmEiJhbhcXkExwz5dyu6jJ2hcK5wP/5lKSKB6VVSYtQSWf2ocp3rZviAiZ9C9RW0GpSRgtcGTE53U26JWc4hLNpLwjVOrfj6pFCUW4lLFJVbu+3ola3fnEBsexPhbO1MjPMjssLzLplmQnWk0w7Kv1xepBp7u35qokADW7s7h0z92OOekWh1iOiUW4jI2m41nJ6czd0MWwQF+fDg0lYax4WaH5X3sS0xTboTAEHNjEXGi2pHBPNEvCYA3Zm5kjzN6W9iT7y3z4MTRqp9PHKbEQlzmgwVb+fLPnVgs8PZ1HejYoBq1n3aXozth00zjuDp22hSfd21qIqkNT/a2mJxe9RPWbgm1k8BapOkQkyixEJeYmraXV6ZtAIzhzsvb1DE5Ii+1/FPABk0uNpbTiVQzfn4WXrqyLQF+Fmat28+M9H1VP6mmQ0ylxEKc7kBuAU/8mAbALRc24rZujU2OyEsVF8KKz4xjjVZINdayzqneFs/+lM6xguKqnbB0OmQu5GdXMTpxlBILcbrnf04n+0QRyQlRPN0/yexwvNfGXyAvCyLioWU/s6MRcamyvS3erGpvi7gkqNUSSgph4zTnBCgVpsRCnGr2uv1MWbMXfz8Lrw5pR4C/fsUqzV602fGfxsZcItVYaJA/L5zsbTF+0TbSdlVxpEHTIabRX31xmtz8Iv7901oAbu/WmDb1ok2OyIsd3GTsDWLxg45DzY5GxC16tKjNFe2N3hZPTFxTtd4W9umQzXMgP8c5AUqFKLEQp3ltxkb2ZufTMDaMB3q3MDsc77ZsnPGx+WUQk2huLCJu9PSApNLeFp9VpbdFXGuIbQ4lBZAx3XkByjkpsRCnWLb9MJ8vNv4IjLyyLaFB6qxZaUUnYNWXxrGKNsXHxEWG8FhfYx+hKvW2sFh8czqkuBD2rzM1BCUWUmUFxSU8NmENNhtck1qfC5vVMjsk75Y+CfKPQkwDaNbL7GhE3O76zg3o2CCGvMISnqtKbwv7dMimWVCQ65zgPNnmOTCmC3w+2NTpHyUWUmXvzdvClgN51IoI5sl+WgVSZfbt0TvdYmwZLuJj/PwsvHyV0dti5rr9zKxsb4v4NlCz6cnpkBnODdKTHNkB39wIX1wFhzaDzWrUaZlEiYVUycZ9uYz5dTMAz1+RTEyY9gGpkr1rYNdS8AuADjebHY2IaVrVieIOe2+LyZXsbVFuOmSS02LzGEX5MP+/8N55sGEKWPzhgnvgvuVQv5NpYSmxkEorsdp4bMIaikps9E6Kp19bddessuUnizaTBkJEnLmxiJjs/kuak1gzlL3Z+fxvViV7W5SbDjnmvODMtnEajD4f5r0ExfnQsBvctRAuHwkh5q7IU2Ihlfb5H9tZlXmUyOAAXhzcBovFYnZI3q0gF9Z8Zxxre3QRo7fFIKO3xbjft7F2dyV6W9RpBzUaGy++9n13vNmhLfDlNfD1dXBkO0TWhSEfwy1TIL612dEBSiykknYfPcF/Z2wE4LG+ragTrV03q2zNd1B4zFgi16ib2dGIeISLW8Yx0N7b4sc0Sqw2x05QXaZDCo/DnBdg9AWwaQb4BULXEXDvMmj7D+P79BBKLMRhNpuNpyamcbywhM6NanDDeQ3MDsn72WynelekDvOoPxIiZvv3gCQiQwJI253NZ39sd/wE9umQjJlQmOfU2FzOZjOWy753Hvz2utGmvElPuHsRXPofCI4wO8K/cSixGDlyJJ07dyYyMpK4uDgGDx7Mxo0bXRWbeKjJq/fw68YDBPn7MfKqdvj56UWwynYtg/1pEBAC7a8zOxoRjxIXGcJjlxu9LV6fsZG92Q72tqibAjENofiEUWvhLQ5kwOdXwnf/hOxMiE6Eaz6HmydCbc9tQuhQYjF//nyGDx/O4sWLmTVrFsXFxfTp04e8PC/LAKXSDucV8vzPRvOV+y5pRrM4z8uWvZJ9X5A2QyCsprmxiHigG85rQIfK9rbwtumQglyY+W+jJ8XWeeAfDN0fheFLoPUVHj+iGeDIg6dPL98Wddy4ccTFxbF8+XK6d+/u1MDEM704ZR2H8wppGR/J//VoanY41cPxw5D+o3GsTpsip+XnZ2HkVW0Z8M5CZqTvZ9a6/VzaOr7iJ2g9CH5/2+hnUXgcgsJcF2xl2WywdgLMfBpy9xr3tbjcWOlRs4m5sTmgSjUW2dlGhW7Nmmd+h1VQUEBOTk65m3in+RkH+HHlbiwWeGVIW4ICVKLjFKu/NirW67SFeuatPRfxdK3qRHH7RSd7W/y0ljxHelskdDS62RYdh82zXRRhFexPh/H9YcJtRlJRoxFc/y3c8K1XJRVQhcTCZrPx0EMP0a1bN9q0aXPGx40cOZLo6OjSW2KiNlTyRnkFxTz5YxoAt17YmA4NapgcUTVhs52aBlHRpsg5jejVnPo1QtnjaG8Li+VUEacnTYecOArTHof3L4Idv0NAKPR8Gu75E1pebnZ0lVLpxOLee+9lzZo1fP3112d93BNPPEF2dnbpLTMzs7KXFBO9MTOD3UdPUC8mlIf7eG7RkNfZ/pvRgjcoEtpebXY0Ih4vNMifFwYbb2Y/cbS3RevBxseN043N/sxktcLKL2FUKvw5BmwlkHQF3LsEevwLAh1fwm+12vjqz508cfJNoFkcqrGwu++++5g8eTILFiygfv36Z31scHAwwcHBlQpOPMPKnUcYt2gbAC9d2Ybw4Er92sjpLD25L0i7ayA40txYRLxEz5Zx9G9Xl1/W7OXJiWlMvKcr/hVZnVavk7GyIjvT2LAraYDrgz2dPatg6r9g1xLj89jm0PfVKm06uH5vDk9NTGPFzqMAXNE+gS5NY6seayU4NGJhs9m49957+fHHH5k7dy6NGzd2VVziIQqLrTzxYxo2G1zZoR4Xt1SbaafJ3W/09wdIvdXcWES8zLMDWhMZHMCaXdl8XtHeFmZPhxw/DFMehLEXG0lFYDj0ft7oSVHJpOJ4YTEjp65nwLsLWbHzKOFB/jwzoDWdG5k3Xe1QYjF8+HC++OILvvrqKyIjI9m3bx/79u3jxAmTh5QAdv5ptDcVpxq7YAsb9uVSMzyIfw/wjHax1cbKz8FaDPXPMwo3RaTC4qJCeLTvyd4WMzPYl51fsSfaE4uN041NvNzBWmI0wHu308maKhu0+Qfctwy6PQABldu8cfa6/Vz65gI+WLCVEquNvm3qMOfhixnWrTEB/uYV1zs0pj1mzBgALr744nL3jxs3jltuucVZMTnOZoPJ98HBDGhxGXS+HZr2Aj+tWqiKzVnHeGeOsXPpMwNaUzNcO5c6jbUEln9qHHfWviAilXHjeQ2YsHwXqzKP8vzP6Yy5qQKrquqlQlQ9yNkNW+ZCq36uDXLXMpj6COxZaXwe1xr6/hcaX1TpU+45eoLnf05nRvp+AOrFhPLC4GQuaeXA8lsXciixsNkc7NHuLvlHISoBDm6EjOnGrUYjo8q+w81qOFQJVquNJ39Mo7DEysUtazMoJcHskKqXzbMheyeE1jj1DkpEHFLa2+LdhUxbu4/Z6/bT+1y9Lfz8jP9zi0cb0yGuSiyOHYA5z8HKL4zPg6Og55PGG1//wEqdsrjEyvhF2/nfrAzyCksI8LNw+0VNuL9XM8KCPKf2rXq8pQ+tAf+cBPcuhwuGG1vGHtkOs56BN1rBxLth13JjZEMq5KslO1my/TBhQf7audQV7EtMU26EwFBzYxHxYkl1o7j9IqPe79nJ6RXrbVE6HTINigucG1BJMfw5FkZ1OpVUtL8B7lsOF9xd6aRiVeZRrhj1Oy/+sp68whI6NazBlPu78XjfVh6VVABYbG4ehsjJySE6Oprs7GyioqJcc5HC47D2B1jyIexbc+r+uilGtthmiGd2XfMQ+7Lz6f3mfI4VFPPswNbc2lVFuk51dCe81Q6wGclwrWZmRyTi1Y4XFnPpmwvYffQEd1zUmKf6n6MezGqF/yVD7h6jCZWz+kXsWGSs9ti/1vi8Tjvo9zo0OL/Sp8w+UcRrMzbw5Z87sdkgOjSQJ/q24prURLfv01TR1+/qMWLxV0Fh0PGf8H8L4PY50P56o9f63lUw+V54MwlmPGXsay/l2Gw2np60lmMFxaQkxvDPLo3MDqn6WfEZYIPGPZRUiDhBWFAAL5b2tth+7t4Wfn7GnhvgnNUhufvgxzthXF8jqQiJgf5vwJ2/VjqpsNlsTF69h95vzueLxUZScVXHesx9uAfXndfAozd/rJ6JhZ3FAvVT4cr34aH1xrKemAZGTcYfo+DdjsbOcRt+MYavxJinXL+fAD8Lrw5pV7G14VJxJUUnEwu0L4iIE/VsFUf/tnUpsdp4amIaJdZzDMbbp0M2TK38dEhJESwaBe+mwppvAQt0ugXuW2GMjvv5V+q02w/m8c9PlnD/1ys5kFtAk9rhfHXH+bx5TQqxEZ7fF8qzJmZcKTzWWNZz4X1GY5SlHxrb526Za9yi6kPqLdBxKET4Zq+G7ONFPPOTsWvgPRc3pWUdNWxyug2/wLH9EBEPrfqbHY1ItfLMwNYsyDjA6l3ZfPnnjrOPuCZeABF14Ng+2DofWvRx7GJbf4WpjxqLBsBovtXvtSrt91NQXMLY+Vt5d95mCoutBAX4cW/PZvxfjyYEB1QuSTFD9R6xOB0/f+MX6Mbv4f6V0HUEhNaEnF0w90V4szX8cBvs+MPnij1fnrqeg8cKaFo7nOGXaIjeJexFmx1urnQRl4icXnxUCI9e3hKA/07fyP6cs/SpqOx0SPYu+G4ofDbISCrCYuGKd+G22VVKKv7Ycoh+b//GG7MyKCy20q1ZLWY80J37ezX3qqQCqmvxpqOK8o1frCUfwu5lp+6PSzZ6DLS7FoIjTAvPHRZtPsgNH/0JwA93dSG1kZboOt3BzUaluMUPRqyBGG3IJ+JsJVYbQ8YsYlXmUfq1rcPoG8/yYr99obGjaEg0PLL57I2qiguMKfQFrxs7pFr8jOmOnk8aKxMr6dCxAl6euoEJK3YBUCsimH8PSOKK9gketxrPt4s3HRUYAu2vgzvmGMU2HW42dpjLSodfHjKWrE79F2RtMDtSlzhRWMITE41Na26+oKGSCldZPs742LyPkgoRF/H3s/DylW3x97MwNW0fc9bvP/ODG3SB8DjIz4ZtC878uE2zYXQXmPMfI6lo0MVYHNDvtUonFVarjW+X7qTXm/OZsGIXFgvcdEED5jzcg0Ep9TwuqXCEEou/SugAg0bBw+vhspFQsykU5sKSsTD6fBg/ANInGUU71cRbczLYceg4dcoMI4qTFZ2AVV8axyraFHGp1glR3NbNWCb/zE/pHC88Q3G+n3+Z6ZCJf//6ke3w9Q3w5RA4vMWojbpyLNw6rUpt+DP253Lt2D94bEIaR48XkVQ3igl3X8iLg9sSHer9U6RKLM4ktAZ0uQfuXQY3T4JWA4yhr+2/wfdD4X9tYN5IyNljdqRVsnZ3Nh/9Zuxc+sLgNkSGeP8vtUda9xOcOALRDaBZb7OjEan2HujdnHoxoew+eoK3Zm868wNLV4f8cuoNY9EJ+PUVeO982PgLWPyhy73G60H7a40Vh5VworCEV6dvoN/bv7F0+xHCgvx5ql8SP9/blY4NzNs0zNl8Z1VIZfn5QdOexi17Fywfb+zxcGwfzH8FFrxmVPd3vh0ad6/0L5wZikusPDZhDSVWG/3b1eXSc7XClcqzF212GlrpJWgiUnFhQQH8Z1Ayt326jI8XbmNwSj1aJ5ymLqBhVwivDXkHYNt8o5Zi+uNGIzuARhcZUx5xSVWKZ96GLP7901p2HTE27by0dTzPXZFMvZjq13lXxZuVUVwI6yfD0o9h56JT99dqYSQY7a8zioE83AfztzBy2gaiQwOZ/VAPakd6/vpor7RvLbzfFfwC4MF1EKkETsRd7vlyOVPT9pGSGMOEuy88fW+eKQ8ayX9oDWNkEYyNyvq8CMlXVukN477sfP4zJZ2pafsASIgO4bkrkumTXKfS5zSLijddKSAI2v4Dhk2DuxdB6m0QFGHsrjrtUaPY8+cRsC/N7EjPaPvBPN6clQHA0/2TlFS4kn20otUAJRUibvbswGQiggNYlXmUr/7ccfoH2adDThwBv0Do9hAMXwJtrqp0UlFitTHu9230fnM+U9P24e9n4Y6LGjProR5emVQ4QiMWzpKfY3ReW/oRHCizeiTxAmMUo/UVEOAZL942m40bPvyTP7YeomuzWL647XyvrkD2aAW5RqJZeAz+ORma9DA7IhGf89kf23nmp3QigwOY/XAP4qNCyj/AWgI/3QvFJ6Dn01Vutb9m11GemriWtJOtxVMSY3j5yrann4rxIhV9/VZi4Ww2G+z43Ugw1v8M1pPVyGG1jP1LUm812oqb6LulmTw6YQ0hgX7MeKA7DWPDTY2nWls2DqY8ALHNjMIvJXAiblditXHVmEWszjxK/7Z1ee/Gji65Tm5+EW/MzOCzP7ZjtUFkSACPXd6KGzx8b4+K0lSIWSwWaNQNrh4PD6ZDz6cgMgGOH4SFb8Lb7eHr62HzbGOHPTfLys3nxV/WAfDQpS2UVLiSzQbLPjaOU4cpqRAxidHbog3+fhZ+SdvL3A1n6W1RCTabjV/W7KXXG/MZv8hIKgalJDDn4R7cdEHDapFUOEKJhStF1oEej8IDaXDN58ZuljYrbJwKXwwxNkFb9C4cP+y2kJ6fvI6c/GLa1ItimLZDd63dy406G/9gY4ddETFNckI0w7o2AuDfk87S28JBOw8d59bxSxn+1QqycgtoFBvG57edx9vXdSAuMuTcJ6iGlFi4g3+AUWMxdDIMXwrn3w3B0XBkG8x82tjGfdJw2L3CpWHMTN/HL2l78fez8MpV7Qjw14/fpexFm22ugjB1MxUx2wO9W5T2tnh7zll6W1RAYbGV9+Zt5tL/zefXjQcI8vfj/l7Nmf5Ady5qXttJEXsn1ViYpTAP0r6HJR/B/jKrRxI6GsWeyVdCUJjTLpeTX8Slb85nf04Bd/VoyuN9Wznt3HIaJ44YRZvF+cbmRImdzY5IRIA56/dz26fL8PezMOW+biTVdfx1aMm2wzw1MY1NWccA6NIklhevbEPT2tV7TynVWHi6oHDodAvc9RvcNsvY6Mw/CPasgJ/ugTdbwbTHnLY/yavTNrA/xxime6B3c6ecU85i1ddGUhHfFuqnmh2NiJzUKymevm3qUGK18cSPaVitFX9vfSSvkEd/WM01H/zBpqxjxIYH8eY17fnqjvOrfVLhCCUWZrNYIPE8uGqs0Typ17PGqpH8bPjzfWN/kk8uhzXfGbuwVsKSbYf58k+ji9zLV7UlJFCdH13KZjs1DZJ6q4o2RTxM2d4WXy7Zec7H22w2vl+WySVv/Mp3y4xdSK8/L5E5D/fgqo71tVz/LzQV4omsVtgy19gNc+M0sJUY94fWhJQbjJGOWhUbdcgvKqHfO7+x9UAe13VO5JUh7VwXtxi2/QafDjCapj28AYIjzY5IRP7i00XbeXay0dtizsM9iPtrb4uTNmfl8tTEtfy5zSiybxkfyUtXtvHJXaA1FeLN/PygeW+47kt4cC1c/CRE1YcTh+GPUTAq1dhlde0Eo734Wbw3bzNbD+RROzKYJ/pWrde9VJB9tKLdNUoqRDzUTRc0pF39aHILinl+yrq/fT2/qITXZ2yk79u/8ee2w4QE+vF431ZMub+bTyYVjtCIhbewlsCmWcaL1qaZwMkfW1gt6HCTsblVzSblnrJhXw4D3llIsdXGmBs70rdtXffH7WuOZcGbrcFaBHctrNLWyiLiWmt3Z3PFqIVYbTDu1s70bBkHwIKMA/z7p7XsOHQcgEtaxfH8Fckk1nReQb030ohFdePnDy0vhxu/M/pidH8UIuoYjbd+fwve6QCfDYZ1k6GkiBKrjccmpFFstdGndTyXt6nevek9xsrPjaSifmclFSIerk296NJ+Pv+etJYdh/K496sV/POTJew4dJw6USG8f1NHPh6a6vNJhSM0YuHNSoogY7rRNnrLXEpHMSLiWVVrIMM3tCUnuO7pe+OL81lL4J0UY7vlwWOMehgR8Wh5BcVc+uZ89mTnY7EYtdd+Fhh6YSMe7tOSiOAAs0P0GBV9/da/mDfzD4Skgcbt8DZY8Sms/AKO7Sfl2Ef8Fmxhb1w34veUQHgfo1GXuM6WuUZSERJj9CEREY8XHhzAfwa14fbPlmGzQbv60bx8ZVva1Is2OzSvpRGLasZWXMCo99+h4/6JdPVPP/WFqHrGJmgdboboeuYFWJ19dR1kTIMLhsPlL5sdjYg4YOLKXVitMLhDPfx9bG+PitLupj5q4spdPPjtaoIC/Jg9tB4Ntn0HK780VpQAWPygxeXGplhNLzFqN6TqjmbC2+2MvWDuXVbh5cAiIt5CUyE+6NCxAv7zs7FsakSv5jRo3gyat4OeTxtbuC8fZ2zpvnGqcYtuAJ1OjmJEqrizSlZ8ZiQVjS5SUiEiPk2rQqqRF6as48jxIlrVieTO7mWWngaGQLur4dapMHyJsQlaSDRk74S5L8L/kuHbm40aARO2cvd6JUVGYgHQ+TZzYxERMZkSi2pi3sYsJq3ag58FXh3SjsAz7VxauyX0fQUe3giD34f654G1GNZPhs+vNLZyX/gWHDvg1vi92sapcGwfhMdBy/5mRyMiYiolFtXAsYJinvrR2CF1WNfGtE+MOfeTAkMh5Xq4fRbcvQg63wHBUcZW7rOfNbZy//5W2LbAWH8lZ2bvtNnxZggIMjcWERGTKbGoBl6fsZE92fnUrxHKQ31aOH6C+GTo/7qxr8UV7xpbt1uLIP1H+HQgjOoMi0bB8cPOD97bHdoCW38FLNBxqNnRiIiYTomFl1ux8wif/rEdgJevbEtYUBXqcYPCjSWpd86DO+cbm50FRcChTTDzKXijFfx4J+z4Q6MYdvbRiuZ9oEZDc2MREfEASiy8WGGxlccnrMFmg6s61qN7i9rOO3lCCgx82xjFGPA/oz11SQGs+RbGXQ6jL4A/P4ATR513TW9TlA+rvjSOU4eZG4uIiIdQYuHF3p+/hYz9x4gND+Lf/Vu75iLBkcaL5v/9BrfPNTY8CwiFAxtg2qPGKMake2DXMt8bxVj3E5w4Yuw82/xSs6MREfEISiy81OasXEbN3QzAMwNbUyPcxUWDFgvU7wSD3jNGMfq+BnGtofiE8a79o17w/kWw9CPIz3FtLJ7CPg3S6RY1GhMROUmJhReynty5tLDEyiWt4riifYJ7AwiNgfPvNFaTDJsJ7a4D/2DYnwa/PGyMYky+H3avqL6jGPvTIXMx+AUYq0FERARQ502v9OWfO1i+4wjhQf68MLgNFotJfe0tFmhwvnG7fCSs/sZ4F39ok7Eh2opPT00TNO8DTXoYBaLVgX20olV/dS0VESlDiYWX2XP0BK9O3wjAo5e3ol5MqMkRnRRWE7rcAxfcbbQNXzYONkyBnF1GK/Hl44xRjUbdjCSjRR+o2eTc5/VEBcdg9bfGsYo2RUTKcXgqZMGCBQwcOJCEhAQsFguTJk1yQVhyOjabjX9PWsuxgmI6Nojhpgs8cHmjxWIkD//4GB7bDjd8D51vN/YlKSmALXNg+mPwTgd4NxWmP2n0gSguNDvyilv7AxTmQs2m0Ki72dGIiHgUh0cs8vLyaN++PbfeeitDhgxxRUyVsmjzQWLCgmgRH0HAmdpZe7lf0vYyZ0MWgf4WXhnSzvO39g0MNUYmWvSBfjY4sBE2zTRuO/8wpkwObYLF7xn9MppcDC0uM0Y0PHV6wWaDpR8bx6nDwK96/q6JiFSWw4lF37596du3rytiqZInJqax49BxQgP9aVsvmpQGMbSvH0P7xGjqxYSaV4fgJEfyCnlucjoA91zcjBbxkSZH5CCLBeJaGbeu90N+NmyZdyrRyDtgTJ1smGI8vk67k0nGZVCvo+esuti9AvatMaZ1Um4wOxoREY/j8hqLgoICCgoKSj/PyXH+UsTCYiv1a4Ry+FghuQXFLNl+mCXbT7WfrhURTEpiNO3rx5DSIIZ29WKIDgt0ehyu9NLU9Rw8VkjzuAju6dnU7HCqLiQakgcbN6sV9q4yEoyMGbDn5Iv3vjWw4DUIi4VmvY2RjKaXGPUcZrEXbSZfaW4cIiIeymKzVX49oMViYeLEiQwePPiMj3nuued4/vnn/3Z/dnY2UVFRlb30aVmtNrYePMaqzGxWZR5hdWY26/fmUGz9+7fYpFY4KYkxtD95S6obSXCAh7wr/ouFmw5y08d/YrHAD3ddSKeGNcwOybWOZcHm2UaSsWUuFJRJRi1+kHi+kWQ072Psc+Ku0agTR+CNJKN3x7CZxmoYEREfkZOTQ3R09Dlfv12eWJxuxCIxMdElicXp5BeVkL4nh9WZR1m96yirMo+y49Dxvz0uyN+PpIQoUuqfmkZpXCvc9CmUE4Ul9HlrPpmHTzC0S0OeH9TG1HjcrqQIMv80koxNs+DA+vJfd+dy1sVjYPrjEN8G7lrovoRGRMQDeExiUdnAXOlIXmFpkrE60/h45HjR3x4XFRJA+8QYUk7e2ifGUCsi2K2xvjx1PWMXbKVudAizHupBRLCPrxA+suNkXcYs2DYfivNPfc2Vy1ltNnjvPDiYAf3fMFa6iIj4ECUWDrDZbGQePsGqXUdZtdMY2Vi7O5uCYuvfHlsvJpSUBjGk1DcSjbb1ogkNcs0UStqubAa9txCrDT4emkqvpHiXXMdrFZ2Abb+dTDRmwNGd5b8e2/xUktHgQgioQtvz7QthfH8IDDdamod4xu+uiIi7VPT12+G3v8eOHWPz5s2ln2/bto1Vq1ZRs2ZNGjRoULloTWaxWGgQG0aD2LDS9thFJVY27sstN6qx+cAxdh89we6jJ/hlzV4A/P0stIiPJCUxunRUo3lcZJWXghaVWHlswhqsNhjYPkFJxemUXc5qe+0cy1kjoenFp2ozHF3Oai/abHe1kgoRkbNweMTi119/pWfPnn+7f+jQoYwfP/6cz/fEEYuKys0vIm13drlkY39Owd8eFxZ0csnryUQjJTGGutEhDtVrjP51M/+dvpGYsEBmP9TD7VMwXq/cctZZkJdV/uuOLGc9lgVvtgZrEfzfAqjb3rWxi4h4ILdMhVSGNycWp7MvO59VmafqNdbsOkpeYcnfHlc7MvhUrUb9GNolRhMVcvolr9sO5nHZWwsoLLbyxtXtGdKpvqu/jeqt7HLWTTONXhSU+bUvu5y1WS8I/cuqm9/ehDnPQ71UuGOOOyMXEfEYSixMUmK1seXAsXKjGhv25VJymiWvTWuH0z4xhg4nRzZa1YkiwM/C9R8u5s9th7moeS0+G3ae6StTqp1jB2DzLCPJ2DwXCrJPfa3sctYWl0HtVvBOilG/MWg0dLjRtLBFRMykxMKDnCgsYd3ebFbuPMrqXdmszjzKzsOnWfIa4Eej2DAy9h8jNNCfmQ92J7FmmAkR+xD7ctZNMyFj5t+Xs4bVguMHjYZeD22AIP08RMQ3uax4UxwXGuRPp4Y16dTwVKfGQ8cKWLMrm5UnRzZW7zrK0eNFZOw/BsDDfVooqXAH/0BjiWqjbnDpf4zlrJtnGUnGtgVGUgGQcqOSChGRCtCIhYew2WzsOHSc1buOUlhsZUjH+vh5+iZj1V3RCWOZ6cEM6DgUgiPMjkhExDSaChERERGnqejrt/Z8FhEREadRYiEiIiJOo8RCREREnEaJhYiIiDiNEgsRERFxGiUWIiIi4jRKLERERMRplFiIiIiI0yixEBEREadRYiEiIiJOo8RCREREnEaJhYiIiDiNEgsRERFxmgB3X9C+mWpOTo67Ly0iIiKVZH/dPtem6G5PLHJzcwFITEx096VFRESkinJzc4mOjj7j1y22c6UeTma1WtmzZw+RkZFYLBannTcnJ4fExEQyMzPPuk+8uId+Hp5HPxPPop+HZ9HP49xsNhu5ubkkJCTg53fmSgq3j1j4+flRv359l50/KipKvxQeRD8Pz6OfiWfRz8Oz6OdxdmcbqbBT8aaIiIg4jRILERERcZpqk1gEBwfz7LPPEhwcbHYogn4enkg/E8+in4dn0c/DedxevCkiIiLVV7UZsRARERHzKbEQERERp1FiISIiIk6jxEJEREScptokFqNHj6Zx48aEhITQqVMnfvvtN7ND8kkjR46kc+fOREZGEhcXx+DBg9m4caPZYclJI0eOxGKx8MADD5gdis/avXs3N910E7GxsYSFhZGSksLy5cvNDstnFRcX8/TTT9O4cWNCQ0Np0qQJ//nPf7BarWaH5rWqRWLx7bff8sADD/DUU0+xcuVKLrroIvr27cvOnTvNDs3nzJ8/n+HDh7N48WJmzZpFcXExffr0IS8vz+zQfN7SpUsZO3Ys7dq1MzsUn3XkyBG6du1KYGAg06ZNY926dbzxxhvExMSYHZrPevXVV3n//fcZNWoU69ev57///S+vvfYa7777rtmhea1qsdz0/PPPp2PHjowZM6b0vqSkJAYPHszIkSNNjEwOHDhAXFwc8+fPp3v37maH47OOHTtGx44dGT16NC+++CIpKSm89dZbZoflcx5//HF+//13jah6kAEDBhAfH8/HH39cet+QIUMICwvj888/NzEy7+X1IxaFhYUsX76cPn36lLu/T58+LFq0yKSoxC47OxuAmjVrmhyJbxs+fDj9+/end+/eZofi0yZPnkxqaipXX301cXFxdOjQgQ8//NDssHxat27dmDNnDhkZGQCsXr2ahQsX0q9fP5Mj815u34TM2Q4ePEhJSQnx8fHl7o+Pj2ffvn0mRSVg7IT30EMP0a1bN9q0aWN2OD7rm2++YcWKFSxdutTsUHze1q1bGTNmDA899BBPPvkkS5Ys4f777yc4OJh//vOfZofnkx577DGys7Np1aoV/v7+lJSU8NJLL3H99debHZrX8vrEwu6vW7DbbDanbssujrv33ntZs2YNCxcuNDsUn5WZmcmIESOYOXMmISEhZofj86xWK6mpqbz88ssAdOjQgfT0dMaMGaPEwiTffvstX3zxBV999RXJycmsWrWKBx54gISEBIYOHWp2eF7J6xOLWrVq4e/v/7fRiaysrL+NYoj73HfffUyePJkFCxZQv359s8PxWcuXLycrK4tOnTqV3ldSUsKCBQsYNWoUBQUF+Pv7mxihb6lbty6tW7cud19SUhITJkwwKSL517/+xeOPP851110HQNu2bdmxYwcjR45UYlFJXl9jERQURKdOnZg1a1a5+2fNmsWFF15oUlS+y2azce+99/Ljjz8yd+5cGjdubHZIPq1Xr16kpaWxatWq0ltqaio33ngjq1atUlLhZl27dv3b8uuMjAwaNmxoUkRy/Phx/PzKvxT6+/truWkVeP2IBcBDDz3EzTffTGpqKl26dGHs2LHs3LmTu+66y+zQfM7w4cP56quv+Omnn4iMjCwdSYqOjiY0NNTk6HxPZGTk3+pbwsPDiY2NVd2LCR588EEuvPBCXn75Za655hqWLFnC2LFjGTt2rNmh+ayBAwfy0ksv0aBBA5KTk1m5ciVvvvkmw4YNMzs072WrJt577z1bw4YNbUFBQbaOHTva5s+fb3ZIPgk47W3cuHFmhyYn9ejRwzZixAizw/BZP//8s61Nmza24OBgW6tWrWxjx441OySflpOTYxsxYoStQYMGtpCQEFuTJk1sTz31lK2goMDs0LxWtehjISIiIp7B62ssRERExHMosRARERGnUWIhIiIiTqPEQkRERJxGiYWIiIg4jRILERERcRolFiIiIuI0SixERETEaZRYiIiIiNMosRARERGnUWIhIiIiTqPEQkRERJzm/wE3GkOKPZewAgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "i = 6\n",
    "phi = 0\n",
    "plt.plot(preds[i, 0, :, phi], label='Predicted')\n",
    "plt.plot(YValidation[i, :, phi], label='Ground Truth')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bc0550",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def standerize2d(data, mean, std):\n",
    "    mean = data.mean(dim=0)\n",
    "    std = data.std(dim=0)\n",
    "    return (data - mean) / std\n",
    "\n",
    "def unstanderize2d(data, mean, std):\n",
    "    return data * std + mean\n",
    "\n",
    "data = torch.rand(100, 10,10) *5 +2\n",
    "mean = data.mean(dim=0)\n",
    "std = data.std(dim=0)\n",
    "std_data = standerize2d(data, mean, std)\n",
    "unstd_data = unstanderize2d(std_data, data.mean(dim=0), data.std(dim=0))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
